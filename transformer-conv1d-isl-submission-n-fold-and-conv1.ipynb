{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cd588db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:05.977355Z",
     "iopub.status.busy": "2023-04-30T17:43:05.976407Z",
     "iopub.status.idle": "2023-04-30T17:43:05.988915Z",
     "shell.execute_reply": "2023-04-30T17:43:05.988003Z"
    },
    "papermill": {
     "duration": 0.030675,
     "end_time": "2023-04-30T17:43:05.991465",
     "exception": false,
     "start_time": "2023-04-30T17:43:05.960790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac61fa17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:06.017212Z",
     "iopub.status.busy": "2023-04-30T17:43:06.016764Z",
     "iopub.status.idle": "2023-04-30T17:43:16.466755Z",
     "shell.execute_reply": "2023-04-30T17:43:16.465316Z"
    },
    "papermill": {
     "duration": 10.466336,
     "end_time": "2023-04-30T17:43:16.469861",
     "exception": false,
     "start_time": "2023-04-30T17:43:06.003525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sn\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.model_selection import train_test_split, GroupShuffleSplit \n",
    "\n",
    "import glob\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import gc\n",
    "import sys\n",
    "import sklearn\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac53b0c5",
   "metadata": {
    "papermill": {
     "duration": 0.011484,
     "end_time": "2023-04-30T17:43:16.494212",
     "exception": false,
     "start_time": "2023-04-30T17:43:16.482728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29300866",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:16.522311Z",
     "iopub.status.busy": "2023-04-30T17:43:16.521096Z",
     "iopub.status.idle": "2023-04-30T17:43:16.528914Z",
     "shell.execute_reply": "2023-04-30T17:43:16.528049Z"
    },
    "papermill": {
     "duration": 0.025261,
     "end_time": "2023-04-30T17:43:16.531328",
     "exception": false,
     "start_time": "2023-04-30T17:43:16.506067",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If True, processing data from scratch\n",
    "# If False, loads preprocessed data\n",
    "PREPROCESS_DATA = False\n",
    "TRAIN_MODEL = True\n",
    "\n",
    "N_FOLDS = 5\n",
    "N_ROWS = 543\n",
    "N_DIMS = 3\n",
    "DIM_NAMES = ['x', 'y', 'z']\n",
    "SEED = 42\n",
    "NUM_CLASSES = 250\n",
    "IS_INTERACTIVE = os.environ['KAGGLE_KERNEL_RUN_TYPE'] == 'Interactive'\n",
    "VERBOSE = 1 if IS_INTERACTIVE else 2\n",
    "\n",
    "INPUT_SIZE = 32\n",
    "\n",
    "NUM_HEADS = 8\n",
    "\n",
    "BATCH_ALL_SIGNS_N = 4\n",
    "BATCH_SIZE = 256\n",
    "N_EPOCHS = 50\n",
    "LR_MAX = 1e-3\n",
    "N_WARMUP_EPOCHS = 0\n",
    "WD_RATIO = 0.05\n",
    "MASK_VAL = 4237"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32417d5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:16.556756Z",
     "iopub.status.busy": "2023-04-30T17:43:16.556341Z",
     "iopub.status.idle": "2023-04-30T17:43:16.797866Z",
     "shell.execute_reply": "2023-04-30T17:43:16.796934Z"
    },
    "papermill": {
     "duration": 0.257624,
     "end_time": "2023-04-30T17:43:16.800716",
     "exception": false,
     "start_time": "2023-04-30T17:43:16.543092",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_SAMPLES: 5000\n"
     ]
    }
   ],
   "source": [
    "# Read Training Data\n",
    "if IS_INTERACTIVE or not PREPROCESS_DATA:\n",
    "    train = pd.read_csv('/kaggle/input/asl-signs/train.csv').sample(int(5e3), random_state=SEED)\n",
    "else:\n",
    "    train = pd.read_csv('/kaggle/input/asl-signs/train.csv')\n",
    "\n",
    "N_SAMPLES = len(train)\n",
    "print(f'N_SAMPLES: {N_SAMPLES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d1726a",
   "metadata": {
    "papermill": {
     "duration": 0.011791,
     "end_time": "2023-04-30T17:43:16.824798",
     "exception": false,
     "start_time": "2023-04-30T17:43:16.813007",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Add File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23c6018",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:16.850238Z",
     "iopub.status.busy": "2023-04-30T17:43:16.849844Z",
     "iopub.status.idle": "2023-04-30T17:43:16.867131Z",
     "shell.execute_reply": "2023-04-30T17:43:16.866080Z"
    },
    "papermill": {
     "duration": 0.032893,
     "end_time": "2023-04-30T17:43:16.869521",
     "exception": false,
     "start_time": "2023-04-30T17:43:16.836628",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prints Shape and Dtype For List Of Variables\n",
    "def print_shape_dtype(l, names):\n",
    "    for e, n in zip(l, names):\n",
    "        print(f'{n} shape: {e.shape}, dtype: {e.dtype}')\n",
    "        \n",
    "def get_file_path(path):\n",
    "    return f'/kaggle/input/asl-signs/{path}'\n",
    "\n",
    "train['file_path'] = train['path'].apply(get_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc40c02",
   "metadata": {
    "papermill": {
     "duration": 0.01167,
     "end_time": "2023-04-30T17:43:16.893178",
     "exception": false,
     "start_time": "2023-04-30T17:43:16.881508",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Ordinally Encode Sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cda2344",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:16.919111Z",
     "iopub.status.busy": "2023-04-30T17:43:16.918343Z",
     "iopub.status.idle": "2023-04-30T17:43:16.941896Z",
     "shell.execute_reply": "2023-04-30T17:43:16.940550Z"
    },
    "papermill": {
     "duration": 0.039679,
     "end_time": "2023-04-30T17:43:16.944596",
     "exception": false,
     "start_time": "2023-04-30T17:43:16.904917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add ordinally Encoded Sign (assign number to each sign name)\n",
    "train['sign_ord'] = train['sign'].astype('category').cat.codes\n",
    "\n",
    "# Dictionaries to translate sign <-> ordinal encoded sign\n",
    "SIGN2ORD = train[['sign', 'sign_ord']].set_index('sign').squeeze().to_dict()\n",
    "ORD2SIGN = train[['sign_ord', 'sign']].set_index('sign_ord').squeeze().to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458469be",
   "metadata": {
    "papermill": {
     "duration": 0.011657,
     "end_time": "2023-04-30T17:43:16.968122",
     "exception": false,
     "start_time": "2023-04-30T17:43:16.956465",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Process Data Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f647f0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:16.994067Z",
     "iopub.status.busy": "2023-04-30T17:43:16.993608Z",
     "iopub.status.idle": "2023-04-30T17:43:17.000339Z",
     "shell.execute_reply": "2023-04-30T17:43:16.999246Z"
    },
    "papermill": {
     "duration": 0.022681,
     "end_time": "2023-04-30T17:43:17.002603",
     "exception": false,
     "start_time": "2023-04-30T17:43:16.979922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Source: https://www.kaggle.com/competitions/asl-signs/overview/evaluation\n",
    "ROWS_PER_FRAME = 543  # number of landmarks per frame\n",
    "\n",
    "def load_relevant_data_subset(pq_path):\n",
    "    data_columns = ['x', 'y', 'z']\n",
    "    data = pd.read_parquet(pq_path, columns=data_columns)\n",
    "    n_frames = int(len(data) / ROWS_PER_FRAME)\n",
    "    data = data.values.reshape(n_frames, ROWS_PER_FRAME, len(data_columns))\n",
    "    return data.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0253107a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:17.030611Z",
     "iopub.status.busy": "2023-04-30T17:43:17.029885Z",
     "iopub.status.idle": "2023-04-30T17:43:17.279973Z",
     "shell.execute_reply": "2023-04-30T17:43:17.278602Z"
    },
    "papermill": {
     "duration": 0.266653,
     "end_time": "2023-04-30T17:43:17.282809",
     "exception": false,
     "start_time": "2023-04-30T17:43:17.016156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "class PreprocessLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(PreprocessLayer, self).__init__()\n",
    "        \n",
    "    def pad_edge(self, t, repeats, side):\n",
    "        if side == 'LEFT':\n",
    "            return tf.concat((tf.repeat(t[:1], repeats=repeats, axis=0), t), axis=0)\n",
    "        elif side == 'RIGHT':\n",
    "            return tf.concat((t, tf.repeat(t[-1:], repeats=repeats, axis=0)), axis=0)\n",
    "    \n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec(shape=[None,N_ROWS,N_DIMS], dtype=tf.float32),),\n",
    "    )\n",
    "    def call(self, data0):\n",
    "        # Number of Frames in Video\n",
    "        N_FRAMES0 = tf.shape(data0)[0]\n",
    "        # Count non NaN Hand values in each frame\n",
    "        frames_hands_non_nan_sum = tf.math.reduce_sum(\n",
    "                tf.cast(tf.math.is_nan(tf.gather(data0, HAND_IDXS0, axis=1)) == False, tf.int32),\n",
    "                axis=[1, 2],\n",
    "            )\n",
    "        # Get indices of frames with at least 1 non NaN Hand Measurement\n",
    "        non_empty_frames_idxs = tf.where(frames_hands_non_nan_sum > 0)\n",
    "        non_empty_frames_idxs = tf.squeeze(non_empty_frames_idxs, axis=1)\n",
    "        # Gather all frame indices with at least 1 non NaN Hand Measurement\n",
    "        data = tf.gather(data0, non_empty_frames_idxs, axis=0)\n",
    "        \n",
    "        # Cast Indices in float32 to be compatible with Tensorflow Lite\n",
    "        non_empty_frames_idxs = tf.cast(non_empty_frames_idxs, tf.float32) \n",
    "\n",
    "        \n",
    "        # Number of Frames in Filtered Video\n",
    "        N_FRAMES = tf.shape(data)[0]\n",
    "        \n",
    "        # Gather Relevant Landmark Columns\n",
    "        data = tf.gather(data, LANDMARK_IDXS0, axis=1)\n",
    "        \n",
    "        # Video fits in INPUT_SIZE\n",
    "        if N_FRAMES < INPUT_SIZE:\n",
    "            # Pad With -1 to indicate padding\n",
    "            non_empty_frames_idxs = tf.pad(non_empty_frames_idxs, [[0, INPUT_SIZE-N_FRAMES]], constant_values=-1)\n",
    "            # Pad Data With Zeros\n",
    "            data = tf.pad(data, [[0, INPUT_SIZE-N_FRAMES], [0,0], [0,0]], constant_values=0)\n",
    "            \n",
    "            data = tf.where(tf.math.equal(data,0.0), np.nan, data)\n",
    "            xyz_ref = tf.gather(data, center_idx, axis=-2)\n",
    "            xyz_ref = tf.reshape(xyz_ref, [-1, N_DIMS])\n",
    "            center_mean = tf.experimental.numpy.nanmean(xyz_ref, axis=0, keepdims=True)\n",
    "            center_std = tf.experimental.numpy.sqrt(tf.experimental.numpy.nanmean((xyz_ref - center_mean)**2, axis=0, keepdims=True))\n",
    "            center_std = tf.math.reduce_mean(tf.gather(center_std, [0,1], axis=-1), axis=1, keepdims=True)\n",
    "            center_mean = tf.reshape(tf.cast(center_mean, tf.float32), [1,1,3])\n",
    "            center_std = tf.reshape(tf.cast(center_std, tf.float32), [1,1,1])\n",
    "            data_normalized = (data - center_mean) / center_std\n",
    "            \n",
    "            # Fill NaN Values With 0\n",
    "            data_normalized = tf.where(tf.math.is_nan(data_normalized), 0.0, data_normalized)\n",
    "            \n",
    "            \n",
    "            return data_normalized, non_empty_frames_idxs\n",
    "        # Video needs to be downsampled to INPUT_SIZE\n",
    "        else:\n",
    "            # Repeat\n",
    "            if N_FRAMES < INPUT_SIZE**2:\n",
    "                repeats = tf.math.floordiv(INPUT_SIZE * INPUT_SIZE, N_FRAMES0)\n",
    "                data = tf.repeat(data, repeats=repeats, axis=0)\n",
    "                non_empty_frames_idxs = tf.repeat(non_empty_frames_idxs, repeats=repeats, axis=0)\n",
    "\n",
    "            # Pad To Multiple Of Input Size\n",
    "            pool_size = tf.math.floordiv(len(data), INPUT_SIZE)\n",
    "            if tf.math.mod(len(data), INPUT_SIZE) > 0:\n",
    "                pool_size += 1\n",
    "\n",
    "            if pool_size == 1:\n",
    "                pad_size = (pool_size * INPUT_SIZE) - len(data)\n",
    "            else:\n",
    "                pad_size = (pool_size * INPUT_SIZE) % len(data)\n",
    "\n",
    "            # Pad Start/End with Start/End value\n",
    "            pad_left = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n",
    "            pad_right = tf.math.floordiv(pad_size, 2) + tf.math.floordiv(INPUT_SIZE, 2)\n",
    "            if tf.math.mod(pad_size, 2) > 0:\n",
    "                pad_right += 1\n",
    "\n",
    "            # Pad By Concatenating Left/Right Edge Values\n",
    "            data = self.pad_edge(data, pad_left, 'LEFT')\n",
    "            data = self.pad_edge(data, pad_right, 'RIGHT')\n",
    "\n",
    "            # Pad Non Empty Frame Indices\n",
    "            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_left, 'LEFT')\n",
    "            non_empty_frames_idxs = self.pad_edge(non_empty_frames_idxs, pad_right, 'RIGHT')\n",
    "\n",
    "            # # better normalization\n",
    "            # data = data - tf.experimental.numpy.nanmean(data, axis=-2)\n",
    "\n",
    "            # Reshape to Mean Pool\n",
    "            data = tf.reshape(data, [INPUT_SIZE, -1, N_COLS, N_DIMS])\n",
    "            non_empty_frames_idxs = tf.reshape(non_empty_frames_idxs, [INPUT_SIZE, -1])\n",
    "\n",
    "            # Mean Pool\n",
    "            data = tf.experimental.numpy.nanmean(data, axis=1)\n",
    "            non_empty_frames_idxs = tf.experimental.numpy.nanmean(non_empty_frames_idxs, axis=1)\n",
    "\n",
    "\n",
    "            data = tf.where(tf.math.equal(data,0.0), np.nan, data)\n",
    "            xyz_ref = tf.gather(data, center_idx, axis=-2)\n",
    "            xyz_ref = tf.reshape(xyz_ref, [-1, N_DIMS])\n",
    "            center_mean = tf.experimental.numpy.nanmean(xyz_ref, axis=0, keepdims=True)\n",
    "            center_std = tf.experimental.numpy.sqrt(tf.experimental.numpy.nanmean((xyz_ref - center_mean)**2, axis=0, keepdims=True))\n",
    "            center_std = tf.math.reduce_mean(tf.gather(center_std, [0,1], axis=-1), axis=1, keepdims=True)\n",
    "            center_mean = tf.reshape(tf.cast(center_mean, tf.float32), [1,1,3])\n",
    "            center_std = tf.reshape(tf.cast(center_std, tf.float32), [1,1,1])\n",
    "            data_normalized = (data - center_mean) / center_std\n",
    "            \n",
    "            # Fill NaN Values With 0\n",
    "            data_normalized = tf.where(tf.math.is_nan(data_normalized), 0.0, data_normalized)\n",
    "            \n",
    "            return data_normalized, non_empty_frames_idxs\n",
    "    \n",
    "preprocess_layer = PreprocessLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97afd6c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:17.308793Z",
     "iopub.status.busy": "2023-04-30T17:43:17.308331Z",
     "iopub.status.idle": "2023-04-30T17:43:17.314248Z",
     "shell.execute_reply": "2023-04-30T17:43:17.313100Z"
    },
    "papermill": {
     "duration": 0.021982,
     "end_time": "2023-04-30T17:43:17.316761",
     "exception": false,
     "start_time": "2023-04-30T17:43:17.294779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_data(file_path):\n",
    "    # Load Raw Data\n",
    "    data = load_relevant_data_subset(file_path)\n",
    "    # Process Data Using Tensorflow\n",
    "    data = preprocess_layer(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4696d7f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:17.345299Z",
     "iopub.status.busy": "2023-04-30T17:43:17.344100Z",
     "iopub.status.idle": "2023-04-30T17:43:17.362571Z",
     "shell.execute_reply": "2023-04-30T17:43:17.361301Z"
    },
    "papermill": {
     "duration": 0.034982,
     "end_time": "2023-04-30T17:43:17.365021",
     "exception": false,
     "start_time": "2023-04-30T17:43:17.330039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# HAND_IDXS: 42, N_COLS: 106\n",
      "LIPS_START: 0, LEFT_HAND_START: 56, RIGHT_HAND_START: 77, POSE_START: 98\n"
     ]
    }
   ],
   "source": [
    "REYE = [\n",
    "    33, 7, 163, 144, 145, 153, 154, 155, 133,\n",
    "    246, 161, 160, 159, 158, 157, 173,\n",
    "]\n",
    "LEYE = [\n",
    "    263, 249, 390, 373, 374, 380, 381, 382, 362,\n",
    "    466, 388, 387, 386, 385, 384, 398,\n",
    "]\n",
    "NOSE=[\n",
    "    1,2,98,327\n",
    "]\n",
    "SLIP = [\n",
    "        78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "        191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "        ]\n",
    "\n",
    "\n",
    "LIPS_IDXS0 = np.array(REYE+LEYE+NOSE+SLIP) #\n",
    "LEFT_HAND_IDXS0 = np.arange(468,489) # 21\n",
    "RIGHT_HAND_IDXS0 = np.arange(522,543) # 21\n",
    "POSE_IDXS0 = np.array([11,13,15,12,14,16,23,24,])+489 # 8\n",
    "\n",
    "LANDMARK_IDXS0 = np.concatenate((LIPS_IDXS0, LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0, POSE_IDXS0))\n",
    "HAND_IDXS0 = np.concatenate((LEFT_HAND_IDXS0, RIGHT_HAND_IDXS0), axis=0)\n",
    "N_COLS = LANDMARK_IDXS0.size\n",
    "# Landmark indices in processed data\n",
    "LIPS_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, LIPS_IDXS0)).squeeze()\n",
    "LEFT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, LEFT_HAND_IDXS0)).squeeze()\n",
    "RIGHT_HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, RIGHT_HAND_IDXS0)).squeeze()\n",
    "HAND_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, HAND_IDXS0)).squeeze()\n",
    "POSE_IDXS = np.argwhere(np.isin(LANDMARK_IDXS0, POSE_IDXS0)).squeeze()\n",
    "\n",
    "print(f'# HAND_IDXS: {len(HAND_IDXS)}, N_COLS: {N_COLS}')\n",
    "\n",
    "LIPS_START = 0\n",
    "LEFT_HAND_START = LIPS_IDXS.size\n",
    "RIGHT_HAND_START = LEFT_HAND_START + LEFT_HAND_IDXS.size\n",
    "POSE_START = RIGHT_HAND_START + RIGHT_HAND_IDXS.size\n",
    "\n",
    "print(f'LIPS_START: {LIPS_START}, LEFT_HAND_START: {LEFT_HAND_START}, RIGHT_HAND_START: {RIGHT_HAND_START}, POSE_START: {POSE_START}')\n",
    "\n",
    "leye_center_idx = np.arange(len(LEYE))\n",
    "reye_center_idx = np.arange(len(REYE)) + len(REYE)\n",
    "nose_center_idx = np.arange(len(NOSE)) + len(REYE) + len(LEYE)\n",
    "lip_center_idx = np.arange(len(SLIP)) + len(REYE)+len(LEYE)+len(NOSE)\n",
    "pose_center_idx = np.array([0,1,6,7]) + POSE_START\n",
    "center_idx = np.array(leye_center_idx.tolist()+reye_center_idx.tolist()+nose_center_idx.tolist()+lip_center_idx.tolist()+pose_center_idx.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1df42d",
   "metadata": {
    "papermill": {
     "duration": 0.011955,
     "end_time": "2023-04-30T17:43:17.389727",
     "exception": false,
     "start_time": "2023-04-30T17:43:17.377772",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1848275",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:17.416261Z",
     "iopub.status.busy": "2023-04-30T17:43:17.415280Z",
     "iopub.status.idle": "2023-04-30T17:43:17.423459Z",
     "shell.execute_reply": "2023-04-30T17:43:17.422275Z"
    },
    "papermill": {
     "duration": 0.0245,
     "end_time": "2023-04-30T17:43:17.426278",
     "exception": false,
     "start_time": "2023-04-30T17:43:17.401778",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Epsilon value for layer normalisation\n",
    "LAYER_NORM_EPS = 1e-6\n",
    "\n",
    "# Dense layer units for landmarks\n",
    "MOTION_UNITS = 128\n",
    "LIPS_UNITS = 256\n",
    "HANDS_UNITS = 256\n",
    "POSE_UNITS = 256\n",
    "# final embedding and transformer embedding size\n",
    "UNITS = 256\n",
    "XYZ_UNITS = 384\n",
    "# Transformer\n",
    "NUM_BLOCKS = 2\n",
    "MLP_RATIO = 2\n",
    "\n",
    "# Dropout\n",
    "EMBEDDING_DROPOUT = 0.00\n",
    "MLP_DROPOUT_RATIO = 0.30\n",
    "CLASSIFIER_DROPOUT_RATIO = 0.10\n",
    "\n",
    "# Initiailizers\n",
    "INIT_HE_UNIFORM = tf.keras.initializers.he_uniform\n",
    "INIT_GLOROT_UNIFORM = tf.keras.initializers.glorot_uniform\n",
    "INIT_ZEROS = tf.keras.initializers.constant(0.0)\n",
    "# Activations\n",
    "GELU = tf.keras.activations.gelu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0237ecc9",
   "metadata": {
    "papermill": {
     "duration": 0.012162,
     "end_time": "2023-04-30T17:43:17.450805",
     "exception": false,
     "start_time": "2023-04-30T17:43:17.438643",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Transformer\n",
    "\n",
    "Need to implement transformer from scratch as TFLite does not support the native TF implementation of MultiHeadAttention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c29243e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:17.477428Z",
     "iopub.status.busy": "2023-04-30T17:43:17.477045Z",
     "iopub.status.idle": "2023-04-30T17:43:17.538148Z",
     "shell.execute_reply": "2023-04-30T17:43:17.536998Z"
    },
    "papermill": {
     "duration": 0.078038,
     "end_time": "2023-04-30T17:43:17.540956",
     "exception": false,
     "start_time": "2023-04-30T17:43:17.462918",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-03-24T17:24:03.552905Z\",\"iopub.execute_input\":\"2023-03-24T17:24:03.553892Z\",\"iopub.status.idle\":\"2023-03-24T17:24:03.560104Z\",\"shell.execute_reply.started\":\"2023-03-24T17:24:03.553842Z\",\"shell.execute_reply\":\"2023-03-24T17:24:03.558879Z\"}}\n",
    "def loss_fn(y_true, y_pred):\n",
    "    loss = tf.keras.losses.categorical_crossentropy(tf.one_hot(y_true, depth=tf.shape(y_pred)[-1])[:,0,:], y_pred, label_smoothing=LABEL_SMOOTHING)\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "# %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-03-24T17:24:04.095699Z\",\"iopub.execute_input\":\"2023-03-24T17:24:04.096388Z\",\"iopub.status.idle\":\"2023-03-24T17:24:04.113226Z\",\"shell.execute_reply.started\":\"2023-03-24T17:24:04.096352Z\",\"shell.execute_reply\":\"2023-03-24T17:24:04.112053Z\"}}\n",
    "def get_model():\n",
    "\n",
    "    def scaled_dot_product(q,k,v, softmax, attention_mask):\n",
    "        #calculates Q . K(transpose)\n",
    "        qkt = tf.matmul(q,k,transpose_b=True)\n",
    "        #caculates scaling factor\n",
    "        dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))\n",
    "        scaled_qkt = qkt/dk\n",
    "        softmax = softmax(scaled_qkt, mask=attention_mask)\n",
    "\n",
    "        z = tf.matmul(softmax,v)\n",
    "        #shape: (m,Tx,depth), same shape as q,k,v\n",
    "        return z\n",
    "\n",
    "    class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "        def __init__(self,d_model,num_of_heads):\n",
    "            super(MultiHeadAttention,self).__init__()\n",
    "            self.d_model = d_model\n",
    "            self.num_of_heads = num_of_heads\n",
    "            self.depth = d_model//num_of_heads\n",
    "            self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "            self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "            self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "            self.wo = tf.keras.layers.Dense(d_model)\n",
    "            self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "        def call(self,x, attention_mask):\n",
    "\n",
    "            multi_attn = []\n",
    "            for i in range(self.num_of_heads):\n",
    "                Q = self.wq[i](x)\n",
    "                K = self.wk[i](x)\n",
    "                V = self.wv[i](x)\n",
    "                multi_attn.append(scaled_dot_product(Q,K,V, self.softmax, attention_mask))\n",
    "\n",
    "            multi_head = tf.concat(multi_attn,axis=-1)\n",
    "            multi_head_attention = self.wo(multi_head)\n",
    "            return multi_head_attention\n",
    "\n",
    "    # %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-03-24T17:24:01.886612Z\",\"iopub.execute_input\":\"2023-03-24T17:24:01.886981Z\",\"iopub.status.idle\":\"2023-03-24T17:24:01.899889Z\",\"shell.execute_reply.started\":\"2023-03-24T17:24:01.886946Z\",\"shell.execute_reply\":\"2023-03-24T17:24:01.898849Z\"}}\n",
    "    # Full Transformer\n",
    "    class Transformer(tf.keras.Model):\n",
    "        def __init__(self, num_blocks):\n",
    "            super(Transformer, self).__init__(name='transformer')\n",
    "            self.num_blocks = num_blocks\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.ln_1s = []\n",
    "            self.mhas = []\n",
    "            self.ln_2s = []\n",
    "            self.mlps = []\n",
    "            # Make Transformer Blocks\n",
    "            for i in range(self.num_blocks):\n",
    "                # First Layer Normalisation\n",
    "                self.ln_1s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n",
    "                # Multi Head Attention\n",
    "                self.mhas.append(MultiHeadAttention(UNITS, NUM_HEADS))\n",
    "                # Second Layer Normalisation\n",
    "                self.ln_2s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n",
    "                # Multi Layer Perception\n",
    "                self.mlps.append(tf.keras.Sequential([\n",
    "                    tf.keras.layers.Dense(384 * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
    "                    LateDropout(MLP_DROPOUT_RATIO),\n",
    "                    tf.keras.layers.Dense(UNITS, kernel_initializer=INIT_HE_UNIFORM),\n",
    "                ]))\n",
    "\n",
    "        def call(self, x, attention_mask):\n",
    "            # Iterate input over transformer blocks\n",
    "            for ln_1, mha, ln_2, mlp in zip(self.ln_1s, self.mhas, self.ln_2s, self.mlps):\n",
    "                x1 = ln_1(x)\n",
    "                attention_output = mha(x1, attention_mask)\n",
    "                x2 = x1 + attention_output\n",
    "                x3 = ln_2(x2)\n",
    "                x3 = mlp(x3)\n",
    "                x = x3 + x2\n",
    "\n",
    "            return x\n",
    "\n",
    "    class LateDropout(tf.keras.layers.Layer):\n",
    "        def __init__(self, rate, noise_shape=None, start_step=160*N_EPOCHS//2, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.rate = rate\n",
    "            self.start_step = start_step\n",
    "            self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            super().build(input_shape)\n",
    "            agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "            self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            if training:\n",
    "                x = tf.cond(self._train_counter < self.start_step, lambda:inputs,  lambda:self.dropout(inputs,training=training))\n",
    "                self._train_counter.assign_add(1)\n",
    "            else:\n",
    "                x = inputs\n",
    "            return x\n",
    "    # %% [markdown]\n",
    "    # # Landmark Embedding\n",
    "\n",
    "    # %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-03-24T17:24:02.513568Z\",\"iopub.execute_input\":\"2023-03-24T17:24:02.514655Z\",\"iopub.status.idle\":\"2023-03-24T17:24:02.523575Z\",\"shell.execute_reply.started\":\"2023-03-24T17:24:02.514603Z\",\"shell.execute_reply\":\"2023-03-24T17:24:02.522523Z\"}}\n",
    "    class LandmarkEmbedding(tf.keras.Model):\n",
    "        def __init__(self, units, name):\n",
    "            super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n",
    "            self.units = units\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            # Embedding for missing landmark in frame, initizlied with zeros\n",
    "            self.empty_embedding = self.add_weight(\n",
    "                name=f'{self.name}_empty_embedding',\n",
    "                shape=[self.units],\n",
    "                initializer=INIT_ZEROS,\n",
    "            )\n",
    "            self.per_cls_embedding = tf.Variable(tf.zeros([self.units], dtype=tf.float32), name='per_cls_embedding')\n",
    "\n",
    "            # Embedding\n",
    "            self.dense = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(384, name=f'{self.name}_dense_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM, activation=GELU),\n",
    "                tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "            ], name=f'{self.name}_dense')\n",
    "\n",
    "        def call(self, x):\n",
    "            return tf.where(\n",
    "                    # Checks whether landmark is missing in frame\n",
    "                    tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n",
    "                    # If so, the empty embedding is used\n",
    "                    self.empty_embedding,\n",
    "                    # Otherwise the landmark data is embedded\n",
    "                    self.dense(x),\n",
    "                ) + self.per_cls_embedding\n",
    "\n",
    "    # %% [markdown]\n",
    "    # # Embedding\n",
    "\n",
    "    # %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-03-24T17:24:03.177702Z\",\"iopub.execute_input\":\"2023-03-24T17:24:03.178116Z\",\"iopub.status.idle\":\"2023-03-24T17:24:03.191425Z\",\"shell.execute_reply.started\":\"2023-03-24T17:24:03.178084Z\",\"shell.execute_reply\":\"2023-03-24T17:24:03.190242Z\"}}\n",
    "    class Embedding(tf.keras.Model):\n",
    "        def __init__(self):\n",
    "            super(Embedding, self).__init__()\n",
    "\n",
    "        def get_diffs(self, l):\n",
    "            S = l.shape[2]\n",
    "            other = tf.expand_dims(l, 3)\n",
    "            other = tf.repeat(other, S, axis=3)\n",
    "            other = tf.transpose(other, [0,1,3,2])\n",
    "            diffs = tf.expand_dims(l, 3) - other\n",
    "            diffs = tf.reshape(diffs, [-1, INPUT_SIZE, S*S])\n",
    "            return diffs\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            # Positional Embedding, initialized with zeros\n",
    "            self.positional_embedding = tf.keras.layers.Embedding(INPUT_SIZE+1, UNITS, embeddings_initializer=INIT_ZEROS)\n",
    "            # Embedding layer for Landmarks\n",
    "            self.motion_embedding = LandmarkEmbedding(MOTION_UNITS, 'motion')\n",
    "\n",
    "            self.lips_embedding = LandmarkEmbedding(LIPS_UNITS, 'lips')\n",
    "            self.left_hand_embedding = LandmarkEmbedding(HANDS_UNITS, 'left_hand')\n",
    "            self.right_hand_embedding = LandmarkEmbedding(HANDS_UNITS, 'right_hand')\n",
    "            self.pose_embedding = LandmarkEmbedding(POSE_UNITS, 'pose')\n",
    "            # Landmark Weights\n",
    "\n",
    "            self.cls_embedding = tf.Variable(tf.zeros([UNITS], dtype=tf.float32), name='cls_embedding')\n",
    "            # self.landmark_weights = tf.Variable(tf.zeros([4], dtype=tf.float32), name='landmark_weights')\n",
    "            # Fully Connected Layers for combined landmarks\n",
    "            self.fc = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(384, name='fully_connected_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM, activation=GELU),\n",
    "                tf.keras.layers.Dense(UNITS, name='fully_connected_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "            ], name='fc')\n",
    "            self.weight = tf.keras.layers.Dense(1, name=f'{self.name}_dense_3', use_bias=False, kernel_initializer=INIT_HE_UNIFORM)\n",
    "            self.dropout = LateDropout(0.2)\n",
    "\n",
    "        def call(self, lips0, left_hand0, right_hand0, pose0, motion0, non_empty_frame_idxs, training=False):\n",
    "            motion_embedding = self.motion_embedding(motion0)\n",
    "            # Lips\n",
    "            lips_embedding = self.lips_embedding(lips0)\n",
    "            w_lips = self.weight(lips_embedding)\n",
    "            # Left Hand\n",
    "            left_hand_embedding = self.left_hand_embedding(left_hand0)\n",
    "            w_left_hand = self.weight(left_hand_embedding)\n",
    "            # Right Hand\n",
    "            right_hand_embedding = self.right_hand_embedding(right_hand0)\n",
    "            w_right_hand = self.weight(right_hand_embedding)\n",
    "            # Pose\n",
    "            # [bs N 2]  # [bs N_frame N 2] # [bs N_frame//SIZE, SIZE, N, 2]\n",
    "            pose_embedding = self.pose_embedding(pose0)\n",
    "            w_pose = self.weight(pose_embedding)\n",
    "            # Merge Embeddings of all landmarks with mean pooling\n",
    "            x = tf.stack((lips_embedding, left_hand_embedding, right_hand_embedding, pose_embedding), axis=3) #[bs, units, 32]\n",
    "            landmark_weights = tf.stack((w_lips, w_left_hand, w_right_hand, w_pose), axis=3) # [bs, 4]\n",
    "            # Merge Landmarks with trainable attention weights\n",
    "            x = x * tf.nn.softmax(landmark_weights, axis=3)\n",
    "            x = tf.reduce_sum(x, axis=3)\n",
    "            x = tf.concat((x, motion_embedding), axis=-1)\n",
    "            # Fully Connected Layers\n",
    "            x = self.fc(x)\n",
    "            x = self.dropout(x) \n",
    "\n",
    "\n",
    "            # Add Positional Embedding\n",
    "            normalised_non_empty_frame_idxs = tf.where(\n",
    "                tf.math.equal(non_empty_frame_idxs, -1.0),\n",
    "                INPUT_SIZE,\n",
    "                tf.cast(\n",
    "                    non_empty_frame_idxs / tf.reduce_max(non_empty_frame_idxs, axis=1, keepdims=True) * INPUT_SIZE,\n",
    "                    tf.int32,\n",
    "                ),\n",
    "            )\n",
    "            x = x + self.positional_embedding(normalised_non_empty_frame_idxs)\n",
    "            x = x + self.cls_embedding\n",
    "            return x\n",
    "    # Inputs\n",
    "    frames = tf.keras.layers.Input([INPUT_SIZE, N_COLS, N_DIMS], dtype=tf.float32, name='frames')\n",
    "    non_empty_frame_idxs = tf.keras.layers.Input([INPUT_SIZE], dtype=tf.float32, name='non_empty_frame_idxs')\n",
    "    # Padding Mask\n",
    "    mask = tf.cast(tf.math.not_equal(non_empty_frame_idxs, -1), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=2)\n",
    "    \n",
    "    \"\"\"\n",
    "        left_hand: 468:489\n",
    "        pose: 489:522\n",
    "        right_hand: 522:543\n",
    "    \"\"\"\n",
    "    x = frames\n",
    "    x = tf.slice(x, [0,0,0,0], [-1,INPUT_SIZE, N_COLS, 2])\n",
    "    left = np.arange(INPUT_SIZE-1)\n",
    "    right = np.arange(1, INPUT_SIZE)\n",
    "    motion = tf.pad(tf.gather(x, left, axis=1) - tf.gather(x, right, axis=1), [[0,0],[0,1],[0,0],[0,0]])\n",
    "    motion = tf.where(tf.math.equal(x, 0.0), 0.0, motion)\n",
    "    motion_dist = tf.math.sqrt(tf.math.reduce_mean(motion**2, axis=-1, keepdims=True))\n",
    "    motion = tf.concat((motion, motion_dist), axis=-1)\n",
    "    motion = tf.reshape(motion, [-1, INPUT_SIZE, 106*3])\n",
    "    # x = tf.concat((x, motion), axis=-1)\n",
    "    # LIPS\n",
    "    lips = tf.slice(x, [0,0,LIPS_START,0], [-1,INPUT_SIZE, 56, 2])\n",
    "    # lips = tf.where(\n",
    "    #         tf.math.equal(lips, 0.0),\n",
    "    #         0.0,\n",
    "    #         (lips - LIPS_MEAN) / LIPS_STD,\n",
    "    #     )\n",
    "    lips = tf.reshape(lips, [-1, INPUT_SIZE, 56*2])\n",
    "    # LEFT HAND\n",
    "    left_hand = tf.slice(x, [0,0,56,0], [-1,INPUT_SIZE, 21, 2])\n",
    "    # left_hand = tf.where(\n",
    "    #         tf.math.equal(left_hand, 0.0),\n",
    "    #         0.0,\n",
    "    #         (left_hand - LEFT_HANDS_MEAN) / LEFT_HANDS_STD,\n",
    "    #     )\n",
    "    left_hand = tf.reshape(left_hand, [-1, INPUT_SIZE, 21*2])\n",
    "    # RIGHT HAND\n",
    "    right_hand = tf.slice(x, [0,0,77,0], [-1,INPUT_SIZE, 21, 2])\n",
    "    # right_hand = tf.where(\n",
    "    #         tf.math.equal(right_hand, 0.0),\n",
    "    #         0.0,\n",
    "    #         (right_hand - RIGHT_HANDS_MEAN) / RIGHT_HANDS_STD,\n",
    "    #     )\n",
    "    right_hand = tf.reshape(right_hand, [-1, INPUT_SIZE, 21*2])\n",
    "    # POSE\n",
    "    pose = tf.slice(x, [0,0,98,0], [-1,INPUT_SIZE, 8, 2])\n",
    "    # pose = tf.where(\n",
    "    #         tf.math.equal(pose, 0.0),\n",
    "    #         0.0,\n",
    "    #         (pose - POSE_MEAN) / POSE_STD,\n",
    "    #     )\n",
    "    pose = tf.reshape(pose, [-1, INPUT_SIZE, 8*2])\n",
    "    \n",
    "    # x = lips, left_hand, right_hand, pose    \n",
    "    x = Embedding()(lips, left_hand, right_hand, pose, motion, non_empty_frame_idxs)\n",
    "    \n",
    "    # Encoder Transformer Blocks\n",
    "    x = Transformer(NUM_BLOCKS)(x, mask) + x\n",
    "    \n",
    "    # Pooling\n",
    "    x = tf.reduce_sum(x * mask, axis=1) / tf.reduce_sum(mask, axis=1)\n",
    "    # Classifier Dropout\n",
    "    x = LateDropout(CLASSIFIER_DROPOUT_RATIO)(x)\n",
    "\n",
    "    # Classification Layer\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.keras.activations.softmax, kernel_initializer=INIT_GLOROT_UNIFORM)(x)\n",
    "    \n",
    "    outputs = x\n",
    "    \n",
    "    # Create Tensorflow Model\n",
    "    model = tf.keras.models.Model(inputs=[frames, non_empty_frame_idxs], outputs=outputs)\n",
    "    \n",
    "    # Adam Optimizer with weight decay\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5, clipnorm=1.0)\n",
    "\n",
    "    # TopK Metrics\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='top_10_acc'),\n",
    "    ]\n",
    "    \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66fee22c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:17.567762Z",
     "iopub.status.busy": "2023-04-30T17:43:17.567368Z",
     "iopub.status.idle": "2023-04-30T17:43:17.616257Z",
     "shell.execute_reply": "2023-04-30T17:43:17.615125Z"
    },
    "papermill": {
     "duration": 0.065921,
     "end_time": "2023-04-30T17:43:17.619121",
     "exception": false,
     "start_time": "2023-04-30T17:43:17.553200",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_model_global():\n",
    "    def scaled_dot_product(q,k,v, softmax, attention_mask):\n",
    "        #calculates Q . K(transpose)\n",
    "        qkt = tf.matmul(q,k,transpose_b=True)\n",
    "        #caculates scaling factor\n",
    "        dk = tf.math.sqrt(tf.cast(q.shape[-1],dtype=tf.float32))\n",
    "        scaled_qkt = qkt/dk\n",
    "        softmax = softmax(scaled_qkt, mask=attention_mask)\n",
    "\n",
    "        z = tf.matmul(softmax,v)\n",
    "        #shape: (m,Tx,depth), same shape as q,k,v\n",
    "        return z\n",
    "\n",
    "    class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "        def __init__(self,d_model,num_of_heads):\n",
    "            super(MultiHeadAttention,self).__init__()\n",
    "            self.d_model = d_model\n",
    "            self.num_of_heads = num_of_heads\n",
    "            self.depth = d_model//num_of_heads\n",
    "            self.wq = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "            self.wk = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "            self.wv = [tf.keras.layers.Dense(self.depth) for i in range(num_of_heads)]\n",
    "            self.wo = tf.keras.layers.Dense(d_model)\n",
    "            self.softmax = tf.keras.layers.Softmax()\n",
    "\n",
    "        def call(self,x, attention_mask):\n",
    "\n",
    "            multi_attn = []\n",
    "            for i in range(self.num_of_heads):\n",
    "                Q = self.wq[i](x)\n",
    "                K = self.wk[i](x)\n",
    "                V = self.wv[i](x)\n",
    "                multi_attn.append(scaled_dot_product(Q,K,V, self.softmax, attention_mask))\n",
    "\n",
    "            multi_head = tf.concat(multi_attn,axis=-1)\n",
    "            multi_head_attention = self.wo(multi_head)\n",
    "            return multi_head_attention\n",
    "\n",
    "    # %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-03-24T17:24:01.886612Z\",\"iopub.execute_input\":\"2023-03-24T17:24:01.886981Z\",\"iopub.status.idle\":\"2023-03-24T17:24:01.899889Z\",\"shell.execute_reply.started\":\"2023-03-24T17:24:01.886946Z\",\"shell.execute_reply\":\"2023-03-24T17:24:01.898849Z\"}}\n",
    "    # Full Transformer\n",
    "    class Transformer(tf.keras.Model):\n",
    "        def __init__(self, num_blocks):\n",
    "            super(Transformer, self).__init__(name='transformer')\n",
    "            self.num_blocks = num_blocks\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            self.ln_1s = []\n",
    "            self.mhas = []\n",
    "            self.ln_2s = []\n",
    "            self.mlps = []\n",
    "            # Make Transformer Blocks\n",
    "            for i in range(self.num_blocks):\n",
    "                # First Layer Normalisation\n",
    "                self.ln_1s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n",
    "                # Multi Head Attention\n",
    "                self.mhas.append(MultiHeadAttention(UNITS, NUM_HEADS))\n",
    "                # Second Layer Normalisation\n",
    "                self.ln_2s.append(tf.keras.layers.LayerNormalization(epsilon=LAYER_NORM_EPS))\n",
    "                # Multi Layer Perception\n",
    "                self.mlps.append(tf.keras.Sequential([\n",
    "                    tf.keras.layers.Dense(384 * MLP_RATIO, activation=GELU, kernel_initializer=INIT_GLOROT_UNIFORM),\n",
    "                    LateDropout(MLP_DROPOUT_RATIO),\n",
    "                    tf.keras.layers.Dense(UNITS, kernel_initializer=INIT_HE_UNIFORM),\n",
    "                ]))\n",
    "\n",
    "        def call(self, x, attention_mask):\n",
    "            # Iterate input over transformer blocks\n",
    "            for ln_1, mha, ln_2, mlp in zip(self.ln_1s, self.mhas, self.ln_2s, self.mlps):\n",
    "                x1 = ln_1(x)\n",
    "                attention_output = mha(x1, attention_mask)\n",
    "                x2 = x1 + attention_output\n",
    "                x3 = ln_2(x2)\n",
    "                x3 = mlp(x3)\n",
    "                x = x3 + x2\n",
    "\n",
    "            return x\n",
    "\n",
    "    class LateDropout(tf.keras.layers.Layer):\n",
    "        def __init__(self, rate, noise_shape=None, start_step=160*N_EPOCHS//2, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.rate = rate\n",
    "            self.start_step = start_step\n",
    "            self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            super().build(input_shape)\n",
    "            agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "            self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            if training:\n",
    "                x = tf.cond(self._train_counter < self.start_step, lambda:inputs,  lambda:self.dropout(inputs,training=training))\n",
    "                self._train_counter.assign_add(1)\n",
    "            else:\n",
    "                x = inputs\n",
    "            return x\n",
    "    # %% [markdown]\n",
    "    # # Landmark Embedding\n",
    "\n",
    "    # %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-03-24T17:24:02.513568Z\",\"iopub.execute_input\":\"2023-03-24T17:24:02.514655Z\",\"iopub.status.idle\":\"2023-03-24T17:24:02.523575Z\",\"shell.execute_reply.started\":\"2023-03-24T17:24:02.514603Z\",\"shell.execute_reply\":\"2023-03-24T17:24:02.522523Z\"}}\n",
    "    class LandmarkEmbedding(tf.keras.Model):\n",
    "        def __init__(self, units, name):\n",
    "            super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n",
    "            self.units = units\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            # Embedding for missing landmark in frame, initizlied with zeros\n",
    "            self.empty_embedding = self.add_weight(\n",
    "                name=f'{self.name}_empty_embedding',\n",
    "                shape=[self.units],\n",
    "                initializer=INIT_ZEROS,\n",
    "            )\n",
    "            self.per_cls_embedding = tf.Variable(tf.zeros([self.units], dtype=tf.float32), name='per_cls_embedding')\n",
    "\n",
    "            # Embedding\n",
    "            self.dense = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(384, name=f'{self.name}_dense_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM, activation=GELU),\n",
    "                tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "            ], name=f'{self.name}_dense')\n",
    "\n",
    "        def call(self, x):\n",
    "            return tf.where(\n",
    "                    # Checks whether landmark is missing in frame\n",
    "                    tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n",
    "                    # If so, the empty embedding is used\n",
    "                    self.empty_embedding,\n",
    "                    # Otherwise the landmark data is embedded\n",
    "                    self.dense(x),\n",
    "                ) + self.per_cls_embedding\n",
    "\n",
    "    # %% [markdown]\n",
    "    # # Embedding\n",
    "\n",
    "    # %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-03-24T17:24:03.177702Z\",\"iopub.execute_input\":\"2023-03-24T17:24:03.178116Z\",\"iopub.status.idle\":\"2023-03-24T17:24:03.191425Z\",\"shell.execute_reply.started\":\"2023-03-24T17:24:03.178084Z\",\"shell.execute_reply\":\"2023-03-24T17:24:03.190242Z\"}}\n",
    "    class Embedding(tf.keras.Model):\n",
    "        def __init__(self):\n",
    "            super(Embedding, self).__init__()\n",
    "\n",
    "        def get_diffs(self, l):\n",
    "            S = l.shape[2]\n",
    "            other = tf.expand_dims(l, 3)\n",
    "            other = tf.repeat(other, S, axis=3)\n",
    "            other = tf.transpose(other, [0,1,3,2])\n",
    "            diffs = tf.expand_dims(l, 3) - other\n",
    "            diffs = tf.reshape(diffs, [-1, INPUT_SIZE, S*S])\n",
    "            return diffs\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            # Positional Embedding, initialized with zeros\n",
    "            self.positional_embedding = tf.keras.layers.Embedding(INPUT_SIZE+1, UNITS, embeddings_initializer=INIT_ZEROS)\n",
    "            # Embedding layer for Landmarks\n",
    "            self.motion_embedding = LandmarkEmbedding(MOTION_UNITS, 'motion')\n",
    "            self.xyz_embedding = LandmarkEmbedding(XYZ_UNITS, 'xyz')\n",
    "            # Landmark Weights\n",
    "\n",
    "            self.cls_embedding = tf.Variable(tf.zeros([UNITS], dtype=tf.float32), name='cls_embedding')\n",
    "            # self.landmark_weights = tf.Variable(tf.zeros([4], dtype=tf.float32), name='landmark_weights')\n",
    "            # Fully Connected Layers for combined landmarks\n",
    "            self.fc = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(384, name='fully_connected_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM, activation=GELU),\n",
    "                tf.keras.layers.Dense(UNITS, name='fully_connected_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "            ], name='fc')\n",
    "            # self.weight = tf.keras.layers.Dense(1, name=f'{self.name}_dense_3', use_bias=False, kernel_initializer=INIT_HE_UNIFORM)\n",
    "            self.dropout = LateDropout(0.2)\n",
    "\n",
    "        def call(self, xyz0, motion0, non_empty_frame_idxs, training=False):\n",
    "            motion_embedding = self.motion_embedding(motion0)\n",
    "            xyz_embedding = self.xyz_embedding(xyz0)\n",
    "            x = tf.concat((xyz_embedding, motion_embedding), axis=-1)\n",
    "            # Fully Connected Layers\n",
    "            x = self.fc(x)\n",
    "            x = self.dropout(x) \n",
    "\n",
    "\n",
    "            # Add Positional Embedding\n",
    "            normalised_non_empty_frame_idxs = tf.where(\n",
    "                tf.math.equal(non_empty_frame_idxs, -1.0),\n",
    "                INPUT_SIZE,\n",
    "                tf.cast(\n",
    "                    non_empty_frame_idxs / tf.reduce_max(non_empty_frame_idxs, axis=1, keepdims=True) * INPUT_SIZE,\n",
    "                    tf.int32,\n",
    "                ),\n",
    "            )\n",
    "            x = x + self.positional_embedding(normalised_non_empty_frame_idxs)\n",
    "            x = x + self.cls_embedding\n",
    "            return x\n",
    "    # Inputs\n",
    "    frames = tf.keras.layers.Input([INPUT_SIZE, N_COLS, N_DIMS], dtype=tf.float32, name='frames')\n",
    "    non_empty_frame_idxs = tf.keras.layers.Input([INPUT_SIZE], dtype=tf.float32, name='non_empty_frame_idxs')\n",
    "    # Padding Mask\n",
    "    mask = tf.cast(tf.math.not_equal(non_empty_frame_idxs, -1), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=2)\n",
    "    \n",
    "    \"\"\"\n",
    "        left_hand: 468:489\n",
    "        pose: 489:522\n",
    "        right_hand: 522:543\n",
    "    \"\"\"\n",
    "    x = frames\n",
    "    x = tf.slice(x, [0,0,0,0], [-1,INPUT_SIZE, N_COLS, 2])\n",
    "    left = np.arange(INPUT_SIZE-1)\n",
    "    right = np.arange(1, INPUT_SIZE)\n",
    "    motion = tf.pad(tf.gather(x, left, axis=1) - tf.gather(x, right, axis=1), [[0,0],[0,1],[0,0],[0,0]])\n",
    "    motion = tf.where(tf.math.equal(x, 0.0), 0.0, motion)\n",
    "    motion_dist = tf.math.sqrt(tf.math.reduce_mean(motion**2, axis=-1, keepdims=True))\n",
    "    motion = tf.concat((motion, motion_dist), axis=-1)\n",
    "    motion = tf.reshape(motion, [-1, INPUT_SIZE, 106*3])\n",
    "\n",
    "    xyz = tf.reshape(x, [-1, INPUT_SIZE, N_COLS*2])\n",
    "\n",
    "    x = Embedding()(xyz, motion, non_empty_frame_idxs)\n",
    "    \n",
    "    # Encoder Transformer Blocks\n",
    "    x = Transformer(NUM_BLOCKS)(x, mask) + x\n",
    "    \n",
    "    # Pooling\n",
    "    x = tf.reduce_sum(x * mask, axis=1) / tf.reduce_sum(mask, axis=1)\n",
    "    # Classifier Dropout\n",
    "    x = LateDropout(CLASSIFIER_DROPOUT_RATIO)(x)\n",
    "\n",
    "    # Classification Layer\n",
    "    x = tf.keras.layers.Dense(NUM_CLASSES, activation=tf.keras.activations.softmax, kernel_initializer=INIT_GLOROT_UNIFORM)(x)\n",
    "    \n",
    "    outputs = x\n",
    "    \n",
    "    # Create Tensorflow Model\n",
    "    model = tf.keras.models.Model(inputs=[frames, non_empty_frame_idxs], outputs=outputs)\n",
    "    \n",
    "    # Adam Optimizer with weight decay\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5, clipnorm=1.0)\n",
    "\n",
    "    # TopK Metrics\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='top_10_acc'),\n",
    "    ]\n",
    "    \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acedc420",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:17.645226Z",
     "iopub.status.busy": "2023-04-30T17:43:17.644577Z",
     "iopub.status.idle": "2023-04-30T17:43:20.740893Z",
     "shell.execute_reply": "2023-04-30T17:43:20.739495Z"
    },
    "papermill": {
     "duration": 3.112688,
     "end_time": "2023-04-30T17:43:20.743908",
     "exception": false,
     "start_time": "2023-04-30T17:43:17.631220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c40b98ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:20.770150Z",
     "iopub.status.busy": "2023-04-30T17:43:20.769672Z",
     "iopub.status.idle": "2023-04-30T17:43:20.775140Z",
     "shell.execute_reply": "2023-04-30T17:43:20.773860Z"
    },
    "papermill": {
     "duration": 0.021677,
     "end_time": "2023-04-30T17:43:20.777728",
     "exception": false,
     "start_time": "2023-04-30T17:43:20.756051",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a7470e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:20.803642Z",
     "iopub.status.busy": "2023-04-30T17:43:20.803241Z",
     "iopub.status.idle": "2023-04-30T17:43:20.809763Z",
     "shell.execute_reply": "2023-04-30T17:43:20.808646Z"
    },
    "papermill": {
     "duration": 0.022926,
     "end_time": "2023-04-30T17:43:20.812504",
     "exception": false,
     "start_time": "2023-04-30T17:43:20.789578",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    InputLayer, Activation, Conv1D, Dropout, BatchNormalization, \n",
    "    MaxPool1D, GlobalAvgPool1D, Flatten, Reshape,\n",
    "    Conv2D, MaxPool2D, GlobalAvgPool2D, AvgPool1D,\n",
    "    DepthwiseConv1D, Dense, DepthwiseConv2D\n",
    ")\n",
    "from tensorflow.keras.models import Sequential, model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2333fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:20.840190Z",
     "iopub.status.busy": "2023-04-30T17:43:20.839358Z",
     "iopub.status.idle": "2023-04-30T17:43:20.889349Z",
     "shell.execute_reply": "2023-04-30T17:43:20.888040Z"
    },
    "papermill": {
     "duration": 0.067447,
     "end_time": "2023-04-30T17:43:20.892150",
     "exception": false,
     "start_time": "2023-04-30T17:43:20.824703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_conv1d_model():\n",
    "    class LateDropout(tf.keras.layers.Layer):\n",
    "        def __init__(self, rate, noise_shape=None, start_step=160*N_EPOCHS//2, **kwargs):\n",
    "            super().__init__(**kwargs)\n",
    "            self.rate = rate\n",
    "            self.start_step = start_step\n",
    "            self.dropout = tf.keras.layers.Dropout(rate, noise_shape=noise_shape)\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            super().build(input_shape)\n",
    "            agg = tf.VariableAggregation.ONLY_FIRST_REPLICA\n",
    "            self._train_counter = tf.Variable(0, dtype=\"int64\", aggregation=agg, trainable=False)\n",
    "\n",
    "        def call(self, inputs, training=False):\n",
    "            if training:\n",
    "                x = tf.cond(self._train_counter < self.start_step, lambda:inputs,  lambda:self.dropout(inputs,training=training))\n",
    "                self._train_counter.assign_add(1)\n",
    "            else:\n",
    "                x = inputs\n",
    "            return x\n",
    "    # %% [markdown]\n",
    "    # # Landmark Embedding\n",
    "\n",
    "    # %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-03-24T17:24:02.513568Z\",\"iopub.execute_input\":\"2023-03-24T17:24:02.514655Z\",\"iopub.status.idle\":\"2023-03-24T17:24:02.523575Z\",\"shell.execute_reply.started\":\"2023-03-24T17:24:02.514603Z\",\"shell.execute_reply\":\"2023-03-24T17:24:02.522523Z\"}}\n",
    "    class LandmarkEmbedding(tf.keras.Model):\n",
    "        def __init__(self, units, name):\n",
    "            super(LandmarkEmbedding, self).__init__(name=f'{name}_embedding')\n",
    "            self.units = units\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            # Embedding for missing landmark in frame, initizlied with zeros\n",
    "            self.empty_embedding = self.add_weight(\n",
    "                name=f'{self.name}_empty_embedding',\n",
    "                shape=[self.units],\n",
    "                initializer=INIT_ZEROS,\n",
    "            )\n",
    "            self.per_cls_embedding = tf.Variable(tf.zeros([self.units], dtype=tf.float32), name='per_cls_embedding')\n",
    "\n",
    "            # Embedding\n",
    "            self.dense = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(384, name=f'{self.name}_dense_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM, activation=GELU),\n",
    "                tf.keras.layers.Dense(self.units, name=f'{self.name}_dense_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "            ], name=f'{self.name}_dense')\n",
    "\n",
    "        def call(self, x):\n",
    "            return tf.where(\n",
    "                    # Checks whether landmark is missing in frame\n",
    "                    tf.reduce_sum(x, axis=2, keepdims=True) == 0,\n",
    "                    # If so, the empty embedding is used\n",
    "                    self.empty_embedding,\n",
    "                    # Otherwise the landmark data is embedded\n",
    "                    self.dense(x),\n",
    "                ) + self.per_cls_embedding\n",
    "\n",
    "    # %% [markdown]\n",
    "    # # Embedding\n",
    "\n",
    "    # %% [code] {\"execution\":{\"iopub.status.busy\":\"2023-03-24T17:24:03.177702Z\",\"iopub.execute_input\":\"2023-03-24T17:24:03.178116Z\",\"iopub.status.idle\":\"2023-03-24T17:24:03.191425Z\",\"shell.execute_reply.started\":\"2023-03-24T17:24:03.178084Z\",\"shell.execute_reply\":\"2023-03-24T17:24:03.190242Z\"}}\n",
    "    class Embedding(tf.keras.Model):\n",
    "        def __init__(self):\n",
    "            super(Embedding, self).__init__()\n",
    "\n",
    "        def get_diffs(self, l):\n",
    "            S = l.shape[2]\n",
    "            other = tf.expand_dims(l, 3)\n",
    "            other = tf.repeat(other, S, axis=3)\n",
    "            other = tf.transpose(other, [0,1,3,2])\n",
    "            diffs = tf.expand_dims(l, 3) - other\n",
    "            diffs = tf.reshape(diffs, [-1, INPUT_SIZE, S*S])\n",
    "            return diffs\n",
    "\n",
    "        def build(self, input_shape):\n",
    "            # Positional Embedding, initialized with zeros\n",
    "            self.positional_embedding = tf.keras.layers.Embedding(INPUT_SIZE+1, UNITS, embeddings_initializer=INIT_ZEROS)\n",
    "            # Embedding layer for Landmarks\n",
    "            self.motion_embedding = LandmarkEmbedding(MOTION_UNITS, 'motion')\n",
    "\n",
    "            self.lips_embedding = LandmarkEmbedding(LIPS_UNITS, 'lips')\n",
    "            self.left_hand_embedding = LandmarkEmbedding(HANDS_UNITS, 'left_hand')\n",
    "            self.right_hand_embedding = LandmarkEmbedding(HANDS_UNITS, 'right_hand')\n",
    "            self.pose_embedding = LandmarkEmbedding(POSE_UNITS, 'pose')\n",
    "            # Landmark Weights\n",
    "\n",
    "            self.cls_embedding = tf.Variable(tf.zeros([UNITS], dtype=tf.float32), name='cls_embedding')\n",
    "            # self.landmark_weights = tf.Variable(tf.zeros([4], dtype=tf.float32), name='landmark_weights')\n",
    "            # Fully Connected Layers for combined landmarks\n",
    "            self.fc = tf.keras.Sequential([\n",
    "                tf.keras.layers.Dense(384, name='fully_connected_1', use_bias=False, kernel_initializer=INIT_GLOROT_UNIFORM, activation=GELU),\n",
    "                tf.keras.layers.Dense(UNITS, name='fully_connected_2', use_bias=False, kernel_initializer=INIT_HE_UNIFORM),\n",
    "            ], name='fc')\n",
    "            self.weight = tf.keras.layers.Dense(1, name=f'{self.name}_dense_3', use_bias=False, kernel_initializer=INIT_HE_UNIFORM)\n",
    "            self.dropout = LateDropout(0.2)\n",
    "\n",
    "        def call(self, lips0, left_hand0, right_hand0, pose0, motion0, non_empty_frame_idxs, training=False):\n",
    "            motion_embedding = self.motion_embedding(motion0)\n",
    "            # Lips\n",
    "            lips_embedding = self.lips_embedding(lips0)\n",
    "            w_lips = self.weight(lips_embedding)\n",
    "            # Left Hand\n",
    "            left_hand_embedding = self.left_hand_embedding(left_hand0)\n",
    "            w_left_hand = self.weight(left_hand_embedding)\n",
    "            # Right Hand\n",
    "            right_hand_embedding = self.right_hand_embedding(right_hand0)\n",
    "            w_right_hand = self.weight(right_hand_embedding)\n",
    "            # Pose\n",
    "            # [bs N 2]  # [bs N_frame N 2] # [bs N_frame//SIZE, SIZE, N, 2]\n",
    "            pose_embedding = self.pose_embedding(pose0)\n",
    "            w_pose = self.weight(pose_embedding)\n",
    "            # Merge Embeddings of all landmarks with mean pooling\n",
    "            x = tf.stack((lips_embedding, left_hand_embedding, right_hand_embedding, pose_embedding), axis=3) #[bs, units, 32]\n",
    "            landmark_weights = tf.stack((w_lips, w_left_hand, w_right_hand, w_pose), axis=3) # [bs, 4]\n",
    "            # Merge Landmarks with trainable attention weights\n",
    "            x = x * tf.nn.softmax(landmark_weights, axis=3)\n",
    "            x = tf.reduce_sum(x, axis=3)\n",
    "            x = tf.concat((x, motion_embedding), axis=-1)\n",
    "            # Fully Connected Layers\n",
    "            x = self.fc(x)\n",
    "            x = self.dropout(x) \n",
    "\n",
    "\n",
    "            # Add Positional Embedding\n",
    "            normalised_non_empty_frame_idxs = tf.where(\n",
    "                tf.math.equal(non_empty_frame_idxs, -1.0),\n",
    "                INPUT_SIZE,\n",
    "                tf.cast(\n",
    "                    non_empty_frame_idxs / tf.reduce_max(non_empty_frame_idxs, axis=1, keepdims=True) * INPUT_SIZE,\n",
    "                    tf.int32,\n",
    "                ),\n",
    "            )\n",
    "            x = x + self.positional_embedding(normalised_non_empty_frame_idxs)\n",
    "            x = x + self.cls_embedding\n",
    "            return x\n",
    "    # Inputs\n",
    "    frames = tf.keras.layers.Input([INPUT_SIZE, N_COLS, N_DIMS], dtype=tf.float32, name='frames')\n",
    "    non_empty_frame_idxs = tf.keras.layers.Input([INPUT_SIZE], dtype=tf.float32, name='non_empty_frame_idxs')\n",
    "    # Padding Mask\n",
    "    mask = tf.cast(tf.math.not_equal(non_empty_frame_idxs, -1), tf.float32)\n",
    "    mask = tf.expand_dims(mask, axis=2)\n",
    "    \n",
    "    \"\"\"\n",
    "        left_hand: 468:489\n",
    "        pose: 489:522\n",
    "        right_hand: 522:543\n",
    "    \"\"\"\n",
    "    x = frames\n",
    "    x = tf.slice(x, [0,0,0,0], [-1,INPUT_SIZE, N_COLS, 2])\n",
    "    left = np.arange(INPUT_SIZE-1)\n",
    "    right = np.arange(1, INPUT_SIZE)\n",
    "    motion = tf.pad(tf.gather(x, left, axis=1) - tf.gather(x, right, axis=1), [[0,0],[0,1],[0,0],[0,0]])\n",
    "    motion = tf.where(tf.math.equal(x, 0.0), 0.0, motion)\n",
    "    motion_dist = tf.math.sqrt(tf.math.reduce_mean(motion**2, axis=-1, keepdims=True))\n",
    "    motion = tf.concat((motion, motion_dist), axis=-1)\n",
    "    motion = tf.reshape(motion, [-1, INPUT_SIZE, 106*3])\n",
    "    # x = tf.concat((x, motion), axis=-1)\n",
    "    # LIPS\n",
    "    lips = tf.slice(x, [0,0,LIPS_START,0], [-1,INPUT_SIZE, 56, 2])\n",
    "    # lips = tf.where(\n",
    "    #         tf.math.equal(lips, 0.0),\n",
    "    #         0.0,\n",
    "    #         (lips - LIPS_MEAN) / LIPS_STD,\n",
    "    #     )\n",
    "    lips = tf.reshape(lips, [-1, INPUT_SIZE, 56*2])\n",
    "    # LEFT HAND\n",
    "    left_hand = tf.slice(x, [0,0,56,0], [-1,INPUT_SIZE, 21, 2])\n",
    "    # left_hand = tf.where(\n",
    "    #         tf.math.equal(left_hand, 0.0),\n",
    "    #         0.0,\n",
    "    #         (left_hand - LEFT_HANDS_MEAN) / LEFT_HANDS_STD,\n",
    "    #     )\n",
    "    left_hand = tf.reshape(left_hand, [-1, INPUT_SIZE, 21*2])\n",
    "    # RIGHT HAND\n",
    "    right_hand = tf.slice(x, [0,0,77,0], [-1,INPUT_SIZE, 21, 2])\n",
    "    # right_hand = tf.where(\n",
    "    #         tf.math.equal(right_hand, 0.0),\n",
    "    #         0.0,\n",
    "    #         (right_hand - RIGHT_HANDS_MEAN) / RIGHT_HANDS_STD,\n",
    "    #     )\n",
    "    right_hand = tf.reshape(right_hand, [-1, INPUT_SIZE, 21*2])\n",
    "    # POSE\n",
    "    pose = tf.slice(x, [0,0,98,0], [-1,INPUT_SIZE, 8, 2])\n",
    "    # pose = tf.where(\n",
    "    #         tf.math.equal(pose, 0.0),\n",
    "    #         0.0,\n",
    "    #         (pose - POSE_MEAN) / POSE_STD,\n",
    "    #     )\n",
    "    pose = tf.reshape(pose, [-1, INPUT_SIZE, 8*2])\n",
    "    \n",
    "    # x = lips, left_hand, right_hand, pose    \n",
    "    x = Embedding()(lips, left_hand, right_hand, pose, motion, non_empty_frame_idxs)\n",
    "    \n",
    "    do = 0.20\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Reshape((1, 32, 256)))\n",
    "\n",
    "    model.add(Conv2D(256, 1, strides=1, padding='valid', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(DepthwiseConv2D((1,3), strides=1, padding='valid', depth_multiplier=1, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(MaxPool2D((1,2), (1,2)))\n",
    "\n",
    "    model.add(Conv2D(256, 1, strides=1, padding='valid', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(DepthwiseConv2D((1,3), strides=1, padding='valid', depth_multiplier=1, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(256, 1, strides=1, padding='valid', activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(DepthwiseConv2D((1,3), strides=1, padding='valid', depth_multiplier=4, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(GlobalAvgPool2D())\n",
    "    model.add(Dropout(rate=do))\n",
    "\n",
    "    model.add(Dense(768, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(rate=do))\n",
    "\n",
    "    model.add(Dense(768, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(rate=do))\n",
    "\n",
    "    model.add(Dense(250, activation='softmax'))\n",
    "    \n",
    "    outputs = model(x)\n",
    "    \n",
    "    model = tf.keras.models.Model(inputs=[frames, non_empty_frame_idxs], outputs=outputs)\n",
    "\n",
    "    # model = tf.keras.models.Model(inputs=[frames, non_empty_frame_idxs], outputs=x)\n",
    "\n",
    "    # Adam Optimizer with weight decay\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=1e-3, weight_decay=1e-5, clipnorm=1.0)\n",
    "\n",
    "    # TopK Metrics\n",
    "    metrics = [\n",
    "        tf.keras.metrics.SparseCategoricalAccuracy(name='acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=5, name='top_5_acc'),\n",
    "        tf.keras.metrics.SparseTopKCategoricalAccuracy(k=10, name='top_10_acc'),\n",
    "    ]\n",
    "    \n",
    "    model.compile(loss=loss_fn, optimizer=optimizer, metrics=metrics)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a600532b",
   "metadata": {
    "papermill": {
     "duration": 0.011811,
     "end_time": "2023-04-30T17:43:20.916725",
     "exception": false,
     "start_time": "2023-04-30T17:43:20.904914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b5e4c9",
   "metadata": {
    "papermill": {
     "duration": 0.011656,
     "end_time": "2023-04-30T17:43:20.940351",
     "exception": false,
     "start_time": "2023-04-30T17:43:20.928695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission\n",
    "\n",
    "Submission code loosley based on [this notebook](https://www.kaggle.com/code/dschettler8845/gislr-learn-eda-baseline#baseline) by [Darien Schettler\n",
    "](https://www.kaggle.com/dschettler8845)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fe40afa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:20.966540Z",
     "iopub.status.busy": "2023-04-30T17:43:20.966103Z",
     "iopub.status.idle": "2023-04-30T17:43:20.991115Z",
     "shell.execute_reply": "2023-04-30T17:43:20.990097Z"
    },
    "papermill": {
     "duration": 0.041199,
     "end_time": "2023-04-30T17:43:20.993465",
     "exception": false,
     "start_time": "2023-04-30T17:43:20.952266",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreprocessLayerV0(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len):\n",
    "        super(PreprocessLayerV0, self).__init__()\n",
    "        self._max_len = max_len\n",
    "        \n",
    "        self.REF = [500, 501, 512, 513, 159,  386, 13,]\n",
    "\n",
    "        self.LIP = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "        ]\n",
    "        self.LLIP = self.LIP[10::-1] + self.LIP[19:10:-1] + self.LIP[29:19:-1] + self.LIP[39:29:-1]\n",
    "        # LLIP = LIP\n",
    "        self.LHAND = np.arange(468, 489).tolist()\n",
    "        self.RHAND = np.arange(522, 543).tolist()\n",
    "        self.POSE = np.arange(500, 512).tolist()\n",
    "        self.LPOSE = [(i + 1) if (n % 2 == 0) else (i - 1) for n, i in enumerate(self.POSE)]        \n",
    "        \n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec(shape=[None, ROWS_PER_FRAME, 3], dtype=tf.float32),),\n",
    "    )\n",
    "    def call(self, frames):\n",
    "        ref = tf.gather(frames, self.REF, axis=1)\n",
    "        \n",
    "        K = tf.shape(frames)[-1]\n",
    "\n",
    "        frames_flat = tf.reshape(ref, (-1, K))\n",
    "\n",
    "        nnan_idxs = ~tf.reduce_any(tf.math.is_nan(frames_flat), -1)\n",
    "\n",
    "        m = tf.reshape(tf.reduce_mean(frames_flat[nnan_idxs], 0), (1, 1, K))\n",
    "        s = tf.reduce_mean(tf.math.reduce_std(frames_flat[nnan_idxs], 0))\n",
    "\n",
    "        frames = frames - m\n",
    "        frames = frames / s\n",
    "\n",
    "        lhand_fs = ~tf.reduce_any(tf.math.is_nan(tf.gather(frames, self.LHAND, axis=1)), axis=(-2, -1))\n",
    "        rhand_fs = ~tf.reduce_any(tf.math.is_nan(tf.gather(frames, self.RHAND, axis=1)), axis=(-2, -1))\n",
    "        hand_fs = lhand_fs | rhand_fs\n",
    "\n",
    "        lhanded = tf.reduce_sum(tf.cast(lhand_fs, tf.float32)) > tf.reduce_sum(tf.cast(rhand_fs, tf.float32))\n",
    "\n",
    "        lframes = tf.concat([\n",
    "            tf.gather(frames, self.LLIP, axis=1),\n",
    "            tf.gather(frames, self.LHAND, axis=1),    \n",
    "        ], axis=1)\n",
    "\n",
    "        rframes = tf.concat([\n",
    "            tf.gather(frames, self.LIP, axis=1),\n",
    "            tf.gather(frames, self.RHAND, axis=1),    \n",
    "        ], axis=1)\n",
    "\n",
    "        frames = tf.where(lhanded, lframes, rframes)\n",
    "\n",
    "        lMf = tf.constant([[-1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.float32)\n",
    "        rMf = tf.constant([[ 1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.float32)\n",
    "\n",
    "        Mf = tf.where(lhanded, lMf, rMf)\n",
    "\n",
    "        frames = frames @ Mf\n",
    "\n",
    "        sh = tf.shape(frames)\n",
    "        n_frames = sh[0]\n",
    "\n",
    "        if n_frames < self._max_len:\n",
    "            add = self._max_len - n_frames\n",
    "            # add_b = tf.random.uniform((), 0, add, dtype=tf.int32)\n",
    "            # add_b = tf.cast(tf.floor(tf.random.uniform((), 0, add)), tf.int32)\n",
    "            # add_b = tf.cast(tf.floor(add / 2), tf.int32)\n",
    "            add_b = add // 2\n",
    "            add_a = add - add_b\n",
    "            frames = tf.concat([\n",
    "                tf.fill((add_b, sh[1], sh[2]), math.nan),\n",
    "                frames,\n",
    "                tf.fill((add_a, sh[1], sh[2]), math.nan),    \n",
    "            ], axis=0)    \n",
    "        elif n_frames > self._max_len:\n",
    "            f_idxs = tf.cast(tf.math.round(tf.cast(n_frames, dtype=tf.float32) * (tf.range(0, self._max_len, dtype=tf.float32) / self._max_len)), tf.int32)\n",
    "            # f_idxs = tf.sort(tf.random.shuffle(tf.range(0, n_frames, 1))[:self._max_len])\n",
    "            # f_idxs = tf.range(0, n_frames, 1)[:self._max_len]\n",
    "            frames = tf.gather(frames, f_idxs, axis=0)\n",
    "\n",
    "        out_frames = frames\n",
    "        \n",
    "        out_frames = tf.where(tf.math.is_nan(out_frames), 0.0, out_frames)\n",
    "\n",
    "        return out_frames[..., :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e0e3ae72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:21.020381Z",
     "iopub.status.busy": "2023-04-30T17:43:21.019650Z",
     "iopub.status.idle": "2023-04-30T17:43:21.044717Z",
     "shell.execute_reply": "2023-04-30T17:43:21.043445Z"
    },
    "papermill": {
     "duration": 0.041803,
     "end_time": "2023-04-30T17:43:21.047440",
     "exception": false,
     "start_time": "2023-04-30T17:43:21.005637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreprocessLayerV0Pose(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len):\n",
    "        super(PreprocessLayerV0Pose, self).__init__()\n",
    "        self._max_len = max_len\n",
    "        \n",
    "        self.REF = [500, 501, 512, 513, 159,  386, 13,]\n",
    "\n",
    "        self.LIP = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "        ]\n",
    "        self.LLIP = self.LIP[10::-1] + self.LIP[19:10:-1] + self.LIP[29:19:-1] + self.LIP[39:29:-1]\n",
    "        # LLIP = LIP\n",
    "        self.LHAND = np.arange(468, 489).tolist()\n",
    "        self.RHAND = np.arange(522, 543).tolist()\n",
    "        self.POSE = np.arange(500, 512).tolist()\n",
    "        self.LPOSE = [(i + 1) if (n % 2 == 0) else (i - 1) for n, i in enumerate(self.POSE)]        \n",
    "        \n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec(shape=[None, ROWS_PER_FRAME, 3], dtype=tf.float32),),\n",
    "    )\n",
    "    def call(self, frames):\n",
    "        ref = tf.gather(frames, self.REF, axis=1)\n",
    "        \n",
    "        K = tf.shape(frames)[-1]\n",
    "\n",
    "        frames_flat = tf.reshape(ref, (-1, K))\n",
    "\n",
    "        nnan_idxs = ~tf.reduce_any(tf.math.is_nan(frames_flat), -1)\n",
    "\n",
    "        m = tf.reshape(tf.reduce_mean(frames_flat[nnan_idxs], 0), (1, 1, K))\n",
    "        s = tf.reduce_mean(tf.math.reduce_std(frames_flat[nnan_idxs], 0))\n",
    "\n",
    "        frames = frames - m\n",
    "        frames = frames / s\n",
    "\n",
    "        lhand_fs = ~tf.reduce_any(tf.math.is_nan(tf.gather(frames, self.LHAND, axis=1)), axis=(-2, -1))\n",
    "        rhand_fs = ~tf.reduce_any(tf.math.is_nan(tf.gather(frames, self.RHAND, axis=1)), axis=(-2, -1))\n",
    "        hand_fs = lhand_fs | rhand_fs\n",
    "\n",
    "        lhanded = tf.reduce_sum(tf.cast(lhand_fs, tf.float32)) > tf.reduce_sum(tf.cast(rhand_fs, tf.float32))\n",
    "\n",
    "        lframes = tf.concat([\n",
    "            tf.gather(frames, self.LLIP, axis=1),\n",
    "            tf.gather(frames, self.LHAND, axis=1), \n",
    "            tf.gather(frames, self.LPOSE, axis=1), \n",
    "        ], axis=1)\n",
    "\n",
    "        rframes = tf.concat([\n",
    "            tf.gather(frames, self.LIP, axis=1),\n",
    "            tf.gather(frames, self.RHAND, axis=1),\n",
    "            tf.gather(frames, self.POSE, axis=1), \n",
    "        ], axis=1)\n",
    "\n",
    "        frames = tf.where(lhanded, lframes, rframes)\n",
    "\n",
    "        lMf = tf.constant([[-1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.float32)\n",
    "        rMf = tf.constant([[ 1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.float32)\n",
    "\n",
    "        Mf = tf.where(lhanded, lMf, rMf)\n",
    "\n",
    "        frames = frames @ Mf\n",
    "\n",
    "        sh = tf.shape(frames)\n",
    "        n_frames = sh[0]\n",
    "\n",
    "        if n_frames < self._max_len:\n",
    "            add = self._max_len - n_frames\n",
    "            # add_b = tf.random.uniform((), 0, add, dtype=tf.int32)\n",
    "            # add_b = tf.cast(tf.floor(tf.random.uniform((), 0, add)), tf.int32)\n",
    "            # add_b = tf.cast(tf.floor(add / 2), tf.int32)\n",
    "            add_b = add // 2\n",
    "            add_a = add - add_b\n",
    "            frames = tf.concat([\n",
    "                tf.fill((add_b, sh[1], sh[2]), math.nan),\n",
    "                frames,\n",
    "                tf.fill((add_a, sh[1], sh[2]), math.nan),    \n",
    "            ], axis=0)    \n",
    "        elif n_frames > self._max_len:\n",
    "            f_idxs = tf.cast(tf.math.round(tf.cast(n_frames, dtype=tf.float32) * (tf.range(0, self._max_len, dtype=tf.float32) / self._max_len)), tf.int32)\n",
    "            # f_idxs = tf.sort(tf.random.shuffle(tf.range(0, n_frames, 1))[:self._max_len])\n",
    "            # f_idxs = tf.range(0, n_frames, 1)[:self._max_len]\n",
    "            frames = tf.gather(frames, f_idxs, axis=0)\n",
    "\n",
    "        out_frames = frames\n",
    "        \n",
    "        out_frames = tf.where(tf.math.is_nan(out_frames), 0.0, out_frames)\n",
    "\n",
    "        return out_frames[..., :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "103f9ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:21.075233Z",
     "iopub.status.busy": "2023-04-30T17:43:21.073963Z",
     "iopub.status.idle": "2023-04-30T17:43:21.102743Z",
     "shell.execute_reply": "2023-04-30T17:43:21.101122Z"
    },
    "papermill": {
     "duration": 0.045422,
     "end_time": "2023-04-30T17:43:21.105375",
     "exception": false,
     "start_time": "2023-04-30T17:43:21.059953",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreprocessLayerV0Eyes(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len):\n",
    "        super(PreprocessLayerV0Eyes, self).__init__()\n",
    "        self._max_len = max_len\n",
    "        \n",
    "        self.REF = [500, 501, 512, 513, 159,  386, 13,]\n",
    "\n",
    "        self.LIP = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "        ]\n",
    "        self.LLIP = self.LIP[10::-1] + self.LIP[19:10:-1] + self.LIP[29:19:-1] + self.LIP[39:29:-1]\n",
    "        # LLIP = LIP\n",
    "        self.LHAND = np.arange(468, 489).tolist()\n",
    "        self.RHAND = np.arange(522, 543).tolist()\n",
    "        #self.POSE = np.arange(500, 512).tolist()\n",
    "        #self.LPOSE = [(i + 1) if (n % 2 == 0) else (i - 1) for n, i in enumerate(self.POSE)]    \n",
    "        \n",
    "        self.EYES = [263, 466, 388, 387, 386, 385, 384, 398, 362, 382, 381, 380, 374, 373, 390, 249, 33, 245, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7]\n",
    "        self.LEYES = [33, 245, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7, 263, 466, 388, 387, 386, 385, 384, 398, 362, 382, 381, 380, 374, 373, 390, 249]        \n",
    "        \n",
    "        \n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec(shape=[None, ROWS_PER_FRAME, 3], dtype=tf.float32),),\n",
    "    )\n",
    "    def call(self, frames):\n",
    "        ref = tf.gather(frames, self.REF, axis=1)\n",
    "        \n",
    "        K = tf.shape(frames)[-1]\n",
    "\n",
    "        frames_flat = tf.reshape(ref, (-1, K))\n",
    "\n",
    "        nnan_idxs = ~tf.reduce_any(tf.math.is_nan(frames_flat), -1)\n",
    "\n",
    "        m = tf.reshape(tf.reduce_mean(frames_flat[nnan_idxs], 0), (1, 1, K))\n",
    "        s = tf.reduce_mean(tf.math.reduce_std(frames_flat[nnan_idxs], 0))\n",
    "\n",
    "        frames = frames - m\n",
    "        frames = frames / s\n",
    "\n",
    "        lhand_fs = ~tf.reduce_any(tf.math.is_nan(tf.gather(frames, self.LHAND, axis=1)), axis=(-2, -1))\n",
    "        rhand_fs = ~tf.reduce_any(tf.math.is_nan(tf.gather(frames, self.RHAND, axis=1)), axis=(-2, -1))\n",
    "        hand_fs = lhand_fs | rhand_fs\n",
    "\n",
    "        lhanded = tf.reduce_sum(tf.cast(lhand_fs, tf.float32)) > tf.reduce_sum(tf.cast(rhand_fs, tf.float32))\n",
    "\n",
    "        lframes = tf.concat([\n",
    "            tf.gather(frames, self.LLIP, axis=1),\n",
    "            tf.gather(frames, self.LHAND, axis=1), \n",
    "            tf.gather(frames, self.LEYES, axis=1), \n",
    "        ], axis=1)\n",
    "\n",
    "        rframes = tf.concat([\n",
    "            tf.gather(frames, self.LIP, axis=1),\n",
    "            tf.gather(frames, self.RHAND, axis=1),\n",
    "            tf.gather(frames, self.EYES, axis=1), \n",
    "        ], axis=1)\n",
    "\n",
    "        frames = tf.where(lhanded, lframes, rframes)\n",
    "\n",
    "        lMf = tf.constant([[-1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.float32)\n",
    "        rMf = tf.constant([[ 1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.float32)\n",
    "\n",
    "        Mf = tf.where(lhanded, lMf, rMf)\n",
    "\n",
    "        frames = frames @ Mf\n",
    "\n",
    "        sh = tf.shape(frames)\n",
    "        n_frames = sh[0]\n",
    "\n",
    "        if n_frames < self._max_len:\n",
    "            add = self._max_len - n_frames\n",
    "            # add_b = tf.random.uniform((), 0, add, dtype=tf.int32)\n",
    "            # add_b = tf.cast(tf.floor(tf.random.uniform((), 0, add)), tf.int32)\n",
    "            # add_b = tf.cast(tf.floor(add / 2), tf.int32)\n",
    "            add_b = add // 2\n",
    "            add_a = add - add_b\n",
    "            frames = tf.concat([\n",
    "                tf.fill((add_b, sh[1], sh[2]), math.nan),\n",
    "                frames,\n",
    "                tf.fill((add_a, sh[1], sh[2]), math.nan),    \n",
    "            ], axis=0)    \n",
    "        elif n_frames > self._max_len:\n",
    "            f_idxs = tf.cast(tf.math.round(tf.cast(n_frames, dtype=tf.float32) * (tf.range(0, self._max_len, dtype=tf.float32) / self._max_len)), tf.int32)\n",
    "            # f_idxs = tf.sort(tf.random.shuffle(tf.range(0, n_frames, 1))[:self._max_len])\n",
    "            # f_idxs = tf.range(0, n_frames, 1)[:self._max_len]\n",
    "            frames = tf.gather(frames, f_idxs, axis=0)\n",
    "\n",
    "        out_frames = frames\n",
    "        \n",
    "        out_frames = tf.where(tf.math.is_nan(out_frames), 0.0, out_frames)\n",
    "\n",
    "        return out_frames[..., :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "efb15cd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:21.133022Z",
     "iopub.status.busy": "2023-04-30T17:43:21.132376Z",
     "iopub.status.idle": "2023-04-30T17:43:21.161050Z",
     "shell.execute_reply": "2023-04-30T17:43:21.159706Z"
    },
    "papermill": {
     "duration": 0.046409,
     "end_time": "2023-04-30T17:43:21.164043",
     "exception": false,
     "start_time": "2023-04-30T17:43:21.117634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PreprocessLayerV0EyesSparce(tf.keras.layers.Layer):\n",
    "    def __init__(self, max_len):\n",
    "        super(PreprocessLayerV0EyesSparce, self).__init__()\n",
    "        self._max_len = max_len\n",
    "        \n",
    "        self.REF = [500, 501, 512, 513, 159,  386, 13,]\n",
    "\n",
    "        self.LIP = [\n",
    "            61, 185, 40, 39, 37, 0, 267, 269, 270, 409,\n",
    "            291, 146, 91, 181, 84, 17, 314, 405, 321, 375,\n",
    "            78, 191, 80, 81, 82, 13, 312, 311, 310, 415,\n",
    "            95, 88, 178, 87, 14, 317, 402, 318, 324, 308,\n",
    "        ]\n",
    "        self.LLIP = self.LIP[10::-1] + self.LIP[19:10:-1] + self.LIP[29:19:-1] + self.LIP[39:29:-1]\n",
    "        # LLIP = LIP\n",
    "        self.LHAND = np.arange(468, 489).tolist()\n",
    "        self.RHAND = np.arange(522, 543).tolist()\n",
    "        #self.POSE = np.arange(500, 512).tolist()\n",
    "        #self.LPOSE = [(i + 1) if (n % 2 == 0) else (i - 1) for n, i in enumerate(self.POSE)]    \n",
    "        \n",
    "        self.EYES = [263, 466, 388, 387, 386, 385, 384, 398, 362, 382, 381, 380, 374, 373, 390, 249, 33, 245, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7]\n",
    "        self.LEYES = [33, 245, 161, 160, 159, 158, 157, 173, 133, 155, 154, 153, 145, 144, 163, 7, 263, 466, 388, 387, 386, 385, 384, 398, 362, 382, 381, 380, 374, 373, 390, 249]        \n",
    "        \n",
    "        \n",
    "    @tf.function(\n",
    "        input_signature=(tf.TensorSpec(shape=[None, ROWS_PER_FRAME, 3], dtype=tf.float32),),\n",
    "    )\n",
    "    def call(self, frames):\n",
    "        ref = tf.gather(frames, self.REF, axis=1)\n",
    "        \n",
    "        K = tf.shape(frames)[-1]\n",
    "\n",
    "        frames_flat = tf.reshape(ref, (-1, K))\n",
    "\n",
    "        nnan_idxs = ~tf.reduce_any(tf.math.is_nan(frames_flat), -1)\n",
    "\n",
    "        m = tf.reshape(tf.reduce_mean(frames_flat[nnan_idxs], 0), (1, 1, K))\n",
    "        s = tf.reduce_mean(tf.math.reduce_std(frames_flat[nnan_idxs], 0))\n",
    "\n",
    "        frames = frames - m\n",
    "        frames = frames / s\n",
    "\n",
    "        lhand_fs = ~tf.reduce_any(tf.math.is_nan(tf.gather(frames, self.LHAND, axis=1)), axis=(-2, -1))\n",
    "        rhand_fs = ~tf.reduce_any(tf.math.is_nan(tf.gather(frames, self.RHAND, axis=1)), axis=(-2, -1))\n",
    "        hand_fs = lhand_fs | rhand_fs\n",
    "\n",
    "        lhanded = tf.reduce_sum(tf.cast(lhand_fs, tf.float32)) > tf.reduce_sum(tf.cast(rhand_fs, tf.float32))\n",
    "\n",
    "        lframes = tf.concat([\n",
    "            tf.gather(frames, self.LLIP[::2], axis=1),\n",
    "            tf.gather(frames, self.LHAND, axis=1), \n",
    "            tf.gather(frames, self.LEYES[::2], axis=1), \n",
    "        ], axis=1)\n",
    "\n",
    "        rframes = tf.concat([\n",
    "            tf.gather(frames, self.LIP[::2], axis=1),\n",
    "            tf.gather(frames, self.RHAND, axis=1),\n",
    "            tf.gather(frames, self.EYES[::2], axis=1), \n",
    "        ], axis=1)\n",
    "\n",
    "        frames = tf.where(lhanded, lframes, rframes)\n",
    "\n",
    "        lMf = tf.constant([[-1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.float32)\n",
    "        rMf = tf.constant([[ 1, 0, 0], [0, 1, 0], [0, 0, 1]], dtype=tf.float32)\n",
    "\n",
    "        Mf = tf.where(lhanded, lMf, rMf)\n",
    "\n",
    "        frames = frames @ Mf\n",
    "\n",
    "        sh = tf.shape(frames)\n",
    "        n_frames = sh[0]\n",
    "\n",
    "        if n_frames < self._max_len:\n",
    "            add = self._max_len - n_frames\n",
    "            # add_b = tf.random.uniform((), 0, add, dtype=tf.int32)\n",
    "            # add_b = tf.cast(tf.floor(tf.random.uniform((), 0, add)), tf.int32)\n",
    "            # add_b = tf.cast(tf.floor(add / 2), tf.int32)\n",
    "            add_b = add // 2\n",
    "            add_a = add - add_b\n",
    "            frames = tf.concat([\n",
    "                tf.fill((add_b, sh[1], sh[2]), math.nan),\n",
    "                frames,\n",
    "                tf.fill((add_a, sh[1], sh[2]), math.nan),    \n",
    "            ], axis=0)    \n",
    "        elif n_frames > self._max_len:\n",
    "            f_idxs = tf.cast(tf.math.round(tf.cast(n_frames, dtype=tf.float32) * (tf.range(0, self._max_len, dtype=tf.float32) / self._max_len)), tf.int32)\n",
    "            # f_idxs = tf.sort(tf.random.shuffle(tf.range(0, n_frames, 1))[:self._max_len])\n",
    "            # f_idxs = tf.range(0, n_frames, 1)[:self._max_len]\n",
    "            frames = tf.gather(frames, f_idxs, axis=0)\n",
    "\n",
    "        out_frames = frames\n",
    "        \n",
    "        out_frames = tf.where(tf.math.is_nan(out_frames), 0.0, out_frames)\n",
    "\n",
    "        return out_frames[..., :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20afd6e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:21.190525Z",
     "iopub.status.busy": "2023-04-30T17:43:21.190128Z",
     "iopub.status.idle": "2023-04-30T17:43:21.209941Z",
     "shell.execute_reply": "2023-04-30T17:43:21.208600Z"
    },
    "papermill": {
     "duration": 0.03563,
     "end_time": "2023-04-30T17:43:21.212299",
     "exception": false,
     "start_time": "2023-04-30T17:43:21.176669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_preprocess_layer96 = PreprocessLayerV0(max_len=96)\n",
    "my_preprocess_layer32 = PreprocessLayerV0(max_len=32)\n",
    "my_preprocess_layer32pose = PreprocessLayerV0Pose(max_len=32)\n",
    "my_preprocess_layer32eyes = PreprocessLayerV0Eyes(max_len=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dab7126f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:21.238200Z",
     "iopub.status.busy": "2023-04-30T17:43:21.237783Z",
     "iopub.status.idle": "2023-04-30T17:43:21.246527Z",
     "shell.execute_reply": "2023-04-30T17:43:21.245567Z"
    },
    "papermill": {
     "duration": 0.024754,
     "end_time": "2023-04-30T17:43:21.249059",
     "exception": false,
     "start_time": "2023-04-30T17:43:21.224305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_preprocess_layer32eyes_s =PreprocessLayerV0EyesSparce(max_len=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66bd1cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:21.276196Z",
     "iopub.status.busy": "2023-04-30T17:43:21.275805Z",
     "iopub.status.idle": "2023-04-30T17:43:21.281097Z",
     "shell.execute_reply": "2023-04-30T17:43:21.279862Z"
    },
    "papermill": {
     "duration": 0.022512,
     "end_time": "2023-04-30T17:43:21.283681",
     "exception": false,
     "start_time": "2023-04-30T17:43:21.261169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a83f3f6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:21.309775Z",
     "iopub.status.busy": "2023-04-30T17:43:21.309298Z",
     "iopub.status.idle": "2023-04-30T17:43:21.314820Z",
     "shell.execute_reply": "2023-04-30T17:43:21.313531Z"
    },
    "papermill": {
     "duration": 0.021701,
     "end_time": "2023-04-30T17:43:21.317357",
     "exception": false,
     "start_time": "2023-04-30T17:43:21.295656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6988376a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:21.343161Z",
     "iopub.status.busy": "2023-04-30T17:43:21.342727Z",
     "iopub.status.idle": "2023-04-30T17:43:21.348087Z",
     "shell.execute_reply": "2023-04-30T17:43:21.347049Z"
    },
    "papermill": {
     "duration": 0.020963,
     "end_time": "2023-04-30T17:43:21.350343",
     "exception": false,
     "start_time": "2023-04-30T17:43:21.329380",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv1_models_path = \"/kaggle/input/conv1dmodels/conv1dmodels/conv1dmodels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d8a8aca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:21.376502Z",
     "iopub.status.busy": "2023-04-30T17:43:21.375456Z",
     "iopub.status.idle": "2023-04-30T17:43:26.233563Z",
     "shell.execute_reply": "2023-04-30T17:43:26.232229Z"
    },
    "papermill": {
     "duration": 4.874359,
     "end_time": "2023-04-30T17:43:26.236436",
     "exception": false,
     "start_time": "2023-04-30T17:43:21.362077",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelname = 'model0sm1_r1_midle_v3_all'\n",
    "my_model1 = load_model(f\"{conv1_models_path}/{modelname}.hdf5\")\n",
    "my_model1.load_weights(f\"{conv1_models_path}/{modelname}/best_weights-560.hdf5\")\n",
    "my_preprocess_layer1 = deepcopy(my_preprocess_layer96)\n",
    "\n",
    "modelname = 'model0sm1_r1_midle_v4_maxlen32_all'\n",
    "my_model3 = load_model(f\"{conv1_models_path}/{modelname}.hdf5\")\n",
    "my_model3.load_weights(f\"{conv1_models_path}/{modelname}/best_weights-768.hdf5\")\n",
    "my_preprocess_layer3 = deepcopy(my_preprocess_layer32)\n",
    "\n",
    "modelname = 'model0sm3_r1_midle_v4_maxlen32_withpose_all'\n",
    "my_model4 = load_model(f\"{conv1_models_path}/{modelname}.hdf5\")\n",
    "my_model4.load_weights(f\"{conv1_models_path}/{modelname}/best_weights-280.hdf5\")\n",
    "my_preprocess_layer4 = deepcopy(my_preprocess_layer32pose)\n",
    "\n",
    "modelname = 'model0sm2_r1_midle_v4_maxlen32_test5_frank_v2_all'\n",
    "my_model5 = load_model(f\"{conv1_models_path}/{modelname}.hdf5\")\n",
    "my_model5.load_weights(f\"{conv1_models_path}/{modelname}/best_weights-560.hdf5\")\n",
    "my_preprocess_layer5 = deepcopy(my_preprocess_layer32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3491db2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:26.263089Z",
     "iopub.status.busy": "2023-04-30T17:43:26.261999Z",
     "iopub.status.idle": "2023-04-30T17:43:28.484717Z",
     "shell.execute_reply": "2023-04-30T17:43:28.483267Z"
    },
    "papermill": {
     "duration": 2.239066,
     "end_time": "2023-04-30T17:43:28.487501",
     "exception": false,
     "start_time": "2023-04-30T17:43:26.248435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelname = 'model0sm2_r1_midle_v4_maxlen32_test5_frank_v3_all'\n",
    "my_model6 = load_model(f\"{conv1_models_path}/{modelname}.hdf5\")\n",
    "my_model6.load_weights(f\"{conv1_models_path}/{modelname}/best_weights-768.hdf5\")\n",
    "my_preprocess_layer6 = deepcopy(my_preprocess_layer32eyes)\n",
    "\n",
    "modelname = 'model0sm2_r1_midle_v4_maxlen32_test5_frank_v2_mu0_all'\n",
    "my_model7 = load_model(f\"{conv1_models_path}/{modelname}.hdf5\")\n",
    "my_model7.load_weights(f\"{conv1_models_path}/{modelname}/best_weights-980.hdf5\")\n",
    "my_preprocess_layer7 = deepcopy(my_preprocess_layer32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e2cadc96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:28.513735Z",
     "iopub.status.busy": "2023-04-30T17:43:28.513329Z",
     "iopub.status.idle": "2023-04-30T17:43:29.712732Z",
     "shell.execute_reply": "2023-04-30T17:43:29.711493Z"
    },
    "papermill": {
     "duration": 1.216198,
     "end_time": "2023-04-30T17:43:29.715717",
     "exception": false,
     "start_time": "2023-04-30T17:43:28.499519",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelname = 'model0sm2_r1_midle_v4_maxlen32_test5_frank_v4_pfaug_all'\n",
    "my_model8 = load_model(f\"{conv1_models_path}/{modelname}.hdf5\")\n",
    "my_model8.load_weights(f\"{conv1_models_path}/{modelname}/best_weights-908.hdf5\")\n",
    "my_preprocess_layer8 = deepcopy(my_preprocess_layer32eyes_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9a2752cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:29.742094Z",
     "iopub.status.busy": "2023-04-30T17:43:29.741671Z",
     "iopub.status.idle": "2023-04-30T17:43:33.057208Z",
     "shell.execute_reply": "2023-04-30T17:43:33.055774Z"
    },
    "papermill": {
     "duration": 3.332273,
     "end_time": "2023-04-30T17:43:33.060097",
     "exception": false,
     "start_time": "2023-04-30T17:43:29.727824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modelname = 'model0sm2_r1_midle_v4_maxlen32_test5_frank_v2_smv1_all'\n",
    "my_model5_sm = load_model(f\"{conv1_models_path}/{modelname}.hdf5\")\n",
    "my_model5_sm.load_weights(f\"{conv1_models_path}/{modelname}/best_weights-908.hdf5\")\n",
    "my_preprocess_layer5_sm = deepcopy(my_preprocess_layer32)\n",
    "\n",
    "modelname = 'model0sm2_r1_midle_v4_maxlen32_test5_frank_v3_smv1_all'\n",
    "my_model6_sm = load_model(f\"{conv1_models_path}/{modelname}.hdf5\")\n",
    "my_model6_sm.load_weights(f\"{conv1_models_path}/{modelname}/best_weights-768.hdf5\")\n",
    "my_preprocess_layer6_sm = deepcopy(my_preprocess_layer32eyes)\n",
    "\n",
    "modelname = 'model0sm2_r1_midle_v4_maxlen32_test5_frank_v2_mu0_smv1_all'\n",
    "my_model7_sm = load_model(f\"{conv1_models_path}/{modelname}.hdf5\")\n",
    "my_model7_sm.load_weights(f\"{conv1_models_path}/{modelname}/best_weights-980.hdf5\")\n",
    "my_preprocess_layer7_sm = deepcopy(my_preprocess_layer32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56affeeb",
   "metadata": {
    "papermill": {
     "duration": 0.011661,
     "end_time": "2023-04-30T17:43:33.083934",
     "exception": false,
     "start_time": "2023-04-30T17:43:33.072273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "56caa8b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:33.110373Z",
     "iopub.status.busy": "2023-04-30T17:43:33.109240Z",
     "iopub.status.idle": "2023-04-30T17:43:42.850569Z",
     "shell.execute_reply": "2023-04-30T17:43:42.849255Z"
    },
    "papermill": {
     "duration": 9.75767,
     "end_time": "2023-04-30T17:43:42.853361",
     "exception": false,
     "start_time": "2023-04-30T17:43:33.095691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "model.load_weights(f'/kaggle/input/isl-transformer-5fold/train_fix_aug_50_nannormal_morekp_cls_dp02_res2_motion_unit_192_motiond_drop_time_aug_shear_lite_latedropout_combineparts_fulldata_moreepoch.py_model_0.h5')\n",
    "models.append(model)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "model = get_model()\n",
    "model.load_weights(f'/kaggle/input/isl-transformer-5fold/train_fix_aug_50_nannormal_morekp_cls_dp02_res2_motion_unit_192_motiond_drop_time_aug_shear_lite_latedropout_combineparts_fulldata.py_model_1.h5')\n",
    "models.append(model)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "adv_conv1d_model = get_conv1d_model()\n",
    "adv_conv1d_model.load_weights(\"/kaggle/input/conv1dmodels/isl-mymodel-6-all_ws.hdf5\")\n",
    "models.append(adv_conv1d_model)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "parts_conv_model = get_model_global()\n",
    "parts_conv_model.load_weights(f'/kaggle/input/isl-transformer-5fold/train_fix_aug_50_nannormal_morekp_cls_dp02_res2_motion_unit_192_motiond_drop_time_aug_shear_lite_latedropout_combineparts_lite2_fulldata.py_model_0.h5')\n",
    "models.append(parts_conv_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98ec88aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:42.879435Z",
     "iopub.status.busy": "2023-04-30T17:43:42.879011Z",
     "iopub.status.idle": "2023-04-30T17:43:43.267649Z",
     "shell.execute_reply": "2023-04-30T17:43:43.266415Z"
    },
    "papermill": {
     "duration": 0.404948,
     "end_time": "2023-04-30T17:43:43.270511",
     "exception": false,
     "start_time": "2023-04-30T17:43:42.865563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TFLite model for submission\n",
    "class TFLiteModel(tf.Module):\n",
    "    def __init__(self, \n",
    "        models,\n",
    "        preprocess_layer1, model1,         \n",
    "        preprocess_layer3, model3,\n",
    "        preprocess_layer4, model4,\n",
    "        preprocess_layer5, model5,\n",
    "        # 24.04         \n",
    "        preprocess_layer6, model6,\n",
    "        preprocess_layer7, model7,\n",
    "        preprocess_layer8, model8,         \n",
    "        ):\n",
    "        super(TFLiteModel, self).__init__()\n",
    "\n",
    "        # Load the feature generation and main models\n",
    "        self.preprocess_layer = preprocess_layer\n",
    "        self.model_nfold = models\n",
    "\n",
    "        self.preprocess_layer1 = preprocess_layer1\n",
    "        self.model1 = model1        \n",
    "        self.preprocess_layer3 = preprocess_layer3\n",
    "        self.model3 = model3\n",
    "        self.preprocess_layer4 = preprocess_layer4\n",
    "        self.model4 = model4\n",
    "#         self.preprocess_layer5 = preprocess_layer5\n",
    "#         self.model5 = model5  \n",
    "        \n",
    "        self.preprocess_layer6 = preprocess_layer6\n",
    "        self.model6 = model6\n",
    "\n",
    "        self.preprocess_layer7 = preprocess_layer7\n",
    "        self.model7 = model7\n",
    "        '''        \n",
    "        self.preprocess_layer8 = preprocess_layer8\n",
    "        self.model8 = model8 \n",
    "        '''\n",
    "    \n",
    "    @tf.function(input_signature=[tf.TensorSpec(shape=[None, N_ROWS, N_DIMS], dtype=tf.float32, name='inputs')])\n",
    "    def __call__(self, inputs):\n",
    "\n",
    "        # Preprocess Data\n",
    "        x, non_empty_frame_idxs = self.preprocess_layer(inputs)\n",
    "        # Add Batch Dimension\n",
    "        x = tf.expand_dims(x, axis=0)\n",
    "        non_empty_frame_idxs = tf.expand_dims(non_empty_frame_idxs, axis=0)\n",
    "        # Make Prediction\n",
    "        outputs=[]\n",
    "        for model in self.model_nfold:\n",
    "            outputs.append(model({'frames': x, 'non_empty_frame_idxs': non_empty_frame_idxs })[0, :])\n",
    "        outputs_tr = tf.keras.layers.Average()(outputs[:3])\n",
    "        # Squeeze Output 1x250 -> 250\n",
    "        outputs_5 = outputs[3]\n",
    "        x = self.preprocess_layer1(tf.cast(inputs, dtype=tf.float32))\n",
    "        outputs_1 = self.model1(x[None])[0, :]        \n",
    "        \n",
    "        x = self.preprocess_layer3(tf.cast(inputs, dtype=tf.float32))\n",
    "        outputs_3 = self.model3(x[None])[0, :]\n",
    "        \n",
    "        x = self.preprocess_layer4(tf.cast(inputs, dtype=tf.float32))\n",
    "        outputs_4 = self.model4(x[None])[0, :]           \n",
    "        \n",
    "#         x = self.preprocess_layer5(tf.cast(inputs, dtype=tf.float32))\n",
    "#         outputs_5 = self.model5(x[None])[0, :] \n",
    "        \n",
    "        x = self.preprocess_layer6(tf.cast(inputs, dtype=tf.float32))\n",
    "        outputs_6 = self.model6(x[None])[0, :]   \n",
    "\n",
    "        x = self.preprocess_layer7(tf.cast(inputs, dtype=tf.float32))\n",
    "        outputs_7 = self.model7(x[None])[0, :]  \n",
    "        '''\n",
    "        x = self.preprocess_layer8(tf.cast(inputs, dtype=tf.float32))\n",
    "        outputs_8 = self.model8(x[None])[0, :]      \n",
    "        '''\n",
    "        \n",
    "        # outputs = outputs_tr\n",
    "        #outputs = 0.2*outputs_1 + 0.2*outputs_3 + 0.3*outputs_4 + 0.3*outputs_5 + 0.3*outputs_6 + 1.0*outputs_tr\n",
    "        #outputs = 0.2*outputs_1 + 0.2*outputs_3 + 0.3*outputs_4 + 0.5*outputs_5 + 1.0*outputs_tr\n",
    "        # outputs = 0.2*outputs_1 + 0.2*outputs_3 + 0.3*outputs_4 + 0.3*outputs_5 + 0.3*outputs_6 + 0.3*outputs_7 + 1.0*outputs_tr\n",
    "        outputs = 0.2*outputs_1 + 0.2*outputs_3 + 0.3*outputs_4 + 0.3*outputs_5 + 0.3*outputs_6 + 0.3*outputs_7 + 1.5*outputs_tr\n",
    "        # outputs = 0.2*outputs_1 + 0.2*outputs_3 + 0.3*outputs_4 + 0.3*outputs_5 + 0.3*outputs_6 + 0.3*outputs_7 + 0.3*outputs_8 + 1.1*outputs_tr\n",
    "        # outputs = 0.2*outputs_1 + 0.2*outputs_3 + 0.3*outputs_4 + 0.3*outputs_5 + 0.3*outputs_6 + 1.0*outputs_tr\n",
    "        #outputs = 0.2*outputs_1 + 0.2*outputs_3 + 0.3*outputs_4 + 0.3*outputs_5 + 0.3*outputs_6 + 0.3*outputs_7        \n",
    "        \n",
    "        #outputs = outputs_8\n",
    "        \n",
    "        # Return a dictionary with the output tensor\n",
    "        return {'outputs': outputs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d5f43b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:43.297810Z",
     "iopub.status.busy": "2023-04-30T17:43:43.297383Z",
     "iopub.status.idle": "2023-04-30T17:43:43.303547Z",
     "shell.execute_reply": "2023-04-30T17:43:43.302564Z"
    },
    "papermill": {
     "duration": 0.02339,
     "end_time": "2023-04-30T17:43:43.306053",
     "exception": false,
     "start_time": "2023-04-30T17:43:43.282663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define TF Lite Model\n",
    "tflite_keras_model = TFLiteModel(\n",
    "    models,\n",
    "    my_preprocess_layer1, my_model1,   \n",
    "    my_preprocess_layer3, my_model3,\n",
    "    my_preprocess_layer4, my_model4,\n",
    "    my_preprocess_layer5, my_model5_sm, # Smaller varians of some models\n",
    "    \n",
    "    my_preprocess_layer6, my_model6_sm,\n",
    "    my_preprocess_layer7, my_model7_sm,\n",
    "    my_preprocess_layer8, my_model8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd008b33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:43.336047Z",
     "iopub.status.busy": "2023-04-30T17:43:43.334794Z",
     "iopub.status.idle": "2023-04-30T17:43:55.078563Z",
     "shell.execute_reply": "2023-04-30T17:43:55.077312Z"
    },
    "papermill": {
     "duration": 11.763199,
     "end_time": "2023-04-30T17:43:55.081298",
     "exception": false,
     "start_time": "2023-04-30T17:43:43.318099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demo_prediction: 206, correct: 206\n",
      "demo_prediction: 20, correct: 20\n",
      "demo_prediction: 178, correct: 178\n",
      "demo_prediction: 114, correct: 114\n",
      "demo_prediction: 221, correct: 221\n",
      "demo_prediction: 230, correct: 230\n",
      "demo_prediction: 25, correct: 25\n",
      "demo_prediction: 236, correct: 236\n",
      "demo_prediction: 249, correct: 184\n",
      "demo_prediction: 191, correct: 191\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "test_sample_id = 5\n",
    "for test_sample_id in range(10):\n",
    "    demo_raw_data = load_relevant_data_subset(train['file_path'].values[test_sample_id])\n",
    "#     print(f'demo_raw_data shape: {demo_raw_data.shape}, dtype: {demo_raw_data.dtype}')\n",
    "    demo_output = tflite_keras_model(demo_raw_data)[\"outputs\"]\n",
    "#     print(f'demo_output shape: {demo_output.shape}, dtype: {demo_output.dtype}')\n",
    "    demo_prediction = demo_output.numpy().argmax()\n",
    "    print(f'demo_prediction: {demo_prediction}, correct: {train.iloc[test_sample_id][\"sign_ord\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bd2cbc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:43:55.108026Z",
     "iopub.status.busy": "2023-04-30T17:43:55.107566Z",
     "iopub.status.idle": "2023-04-30T17:47:40.340052Z",
     "shell.execute_reply": "2023-04-30T17:47:40.336760Z"
    },
    "papermill": {
     "duration": 225.250167,
     "end_time": "2023-04-30T17:47:40.343932",
     "exception": false,
     "start_time": "2023-04-30T17:43:55.093765",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: kaggle/working/model.tflite (deflated 9%)\r\n"
     ]
    }
   ],
   "source": [
    "# Create Model Converter\n",
    "keras_model_converter = tf.lite.TFLiteConverter.from_keras_model(tflite_keras_model)\n",
    "keras_model_converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "keras_model_converter.target_spec.supported_types = [tf.float16]\n",
    "# Convert Model\n",
    "tflite_model = keras_model_converter.convert()\n",
    "# Write Model\n",
    "with open('/kaggle/working/model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "    \n",
    "# Zip Model\n",
    "!zip submission.zip /kaggle/working/model.tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0dc6bb47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:47:40.389170Z",
     "iopub.status.busy": "2023-04-30T17:47:40.388466Z",
     "iopub.status.idle": "2023-04-30T17:47:41.620495Z",
     "shell.execute_reply": "2023-04-30T17:47:41.619083Z"
    },
    "papermill": {
     "duration": 1.258811,
     "end_time": "2023-04-30T17:47:41.623862",
     "exception": false,
     "start_time": "2023-04-30T17:47:40.365051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 77268\r\n",
      "---------- 1 root root   117489 Apr 30 17:47 __notebook__.ipynb\r\n",
      "-rw-r--r-- 1 root root 37657261 Apr 30 17:47 submission.zip\r\n",
      "-rw-r--r-- 1 root root 41342592 Apr 30 17:47 model.tflite\r\n"
     ]
    }
   ],
   "source": [
    "!ls -t -l /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bedaf314",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:47:41.691851Z",
     "iopub.status.busy": "2023-04-30T17:47:41.691425Z",
     "iopub.status.idle": "2023-04-30T17:47:41.700810Z",
     "shell.execute_reply": "2023-04-30T17:47:41.699554Z"
    },
    "papermill": {
     "duration": 0.032889,
     "end_time": "2023-04-30T17:47:41.703506",
     "exception": false,
     "start_time": "2023-04-30T17:47:41.670617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41943040"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max size of model\n",
    "40*1024*1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68026726",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-30T17:47:41.730922Z",
     "iopub.status.busy": "2023-04-30T17:47:41.730541Z",
     "iopub.status.idle": "2023-04-30T17:47:41.735893Z",
     "shell.execute_reply": "2023-04-30T17:47:41.734643Z"
    },
    "papermill": {
     "duration": 0.022074,
     "end_time": "2023-04-30T17:47:41.738278",
     "exception": false,
     "start_time": "2023-04-30T17:47:41.716204",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Verify TFLite model can be loaded and used for prediction\n",
    "#!pip install tflite-runtime\n",
    "#import tflite_runtime.interpreter as tflite\n",
    "\n",
    "#interpreter = tflite.Interpreter(\"/kaggle/working/model.tflite\")\n",
    "#found_signatures = list(interpreter.get_signature_list().keys())\n",
    "#prediction_fn = interpreter.get_signature_runner(\"serving_default\")\n",
    "\n",
    "#output = prediction_fn(inputs=demo_raw_data)\n",
    "#sign = output['outputs'].argmax()\n",
    "\n",
    "#print(\"PRED : \", ORD2SIGN.get(sign), f'[{sign}]')\n",
    "#print(\"TRUE : \", train.sign.values[0], f'[{train.sign_ord.values[0]}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5fb6ff",
   "metadata": {
    "papermill": {
     "duration": 0.012369,
     "end_time": "2023-04-30T17:47:41.763273",
     "exception": false,
     "start_time": "2023-04-30T17:47:41.750904",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 289.16708,
   "end_time": "2023-04-30T17:47:44.699842",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-04-30T17:42:55.532762",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
